\documentclass{report}
% Comment the following line to NOT allow the usage of umlauts
\usepackage[utf8]{inputenc}
% Uncomment the following line to allow the usage of graphics (.png, .jpg)
\usepackage{geometry}
\geometry{left=3cm,right=3cm,top=3cm,bottom=3cm}

\usepackage[usenames,dvipsnames]{color}
\usepackage[colorlinks,linkcolor=NavyBlue,anchorcolor=red,citecolor=green]{hyperref}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{float}
\usepackage{bm}
\usepackage{array,makecell}
\usepackage[table,xcdraw]{xcolor}
\usepackage{amsmath,amsfonts,amssymb,mathrsfs}
\usepackage{tikz,tikz-cd}
\usetikzlibrary{arrows.meta,arrows,graphs,graphs.standard,calc,decorations.markings}


\usepackage[thmmarks,amsmath]{ntheorem}
\theorembodyfont{\upshape}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\normalfont}
\theoremsymbol{\\ \rightline{$\square$}}
\newtheorem{proof}{Proof.}

% Start the document
\begin{document}
	  

~\\
\begin{center}	
	~\\ 
	\vspace{6em} 
	\textsc{\Huge Stochastic Process}	
	~\\
	\vspace{2.5em} 
	{\Large }
	~\\
	\vspace{6em}
	\textsf{H.C.}
	~\\
	\vspace{5in}  
	{\large Latest Update: \today}
\end{center}
\tableofcontents
% Create a new 1st level heading
\chapter{Preliminaries}

$n \ge k$ is used as an alternative for the
statement $n\in\mathbb{Z}_{\ge k}=\mathbb{Z}\cap[k,\infty)$. $t \ge s$ is used as an alternative for the statement $t\in\mathbb{R}_{\ge s}=\mathbb{R}\cap[s,\infty)$.
\begin{definition}[stochastic process]
For a given probability space $(\Omega ,{\mathcal{F}},\mathrm{P})$ and a measurable space $(S,\mathcal{S})$, a \emph{stochastic process} is a collection of $S$-valued random variables on $(\Omega ,{\mathcal{F}},\mathrm{P})$ indexed by some set $T$, which can be written as $X=\{X_t:X_t\text{ is a random variable on } (\Omega ,\mathcal{F},\mathrm{P}),t\in T\}$ or $X=(X_t)_{t\in T}$ or $X:\Omega \times T\rightarrow S$. This mathematical space $(S,\mathcal{S})$ is called its state space. 
\end{definition}

Note the identification (up to appropriate bijections) among the collection of mappings $\{X_t\in S^\Omega:\sigma(X_t)\in\mathcal{F},t\in T\}$, the mapping $X_\cdot(-):T \to S^\Omega,t\mapsto\left(\omega\mapsto X_{t}(\omega)\right)$, the mapping $X_\cdot(\cdot):\Omega \times T\rightarrow S,(\omega,t)\mapsto X_t(\omega)$ and the mapping $X_{-}(\cdot):\Omega\to S^T,\omega\mapsto \left(t\mapsto X_{t}(\omega)\right)$, each of which can be denoted by $X$.

The following proposition actually gives an equivalent definition of stochastic process.
\begin{proposition}[measurability of $X:\Omega\to S^T$]
	There is a natural bijection between $S^T$ and $\prod_{t\in T}S_t$
	\begin{align*}
	i:S^T&\longrightarrow \prod_{t\in T}S_t\\
	f&\longmapsto (f(t))_{t\in T}
	\end{align*} 
	Therefore, we can identify $S^T$ and $\prod_{t\in T}S_t$ and then define the $\sigma$-algebra on $S^T$
	\[
	\mathcal{S}^T:=\bigotimes_{i\in T}\mathcal{S}_i.
	\]
	A function $X: \Omega \rightarrow S^T$ is $\mathcal{F}/\mathcal{S}^{T}$-measurable iff $X_{t}: \Omega \rightarrow S$ is $\mathcal{F}/\mathcal{S}$-measurable for every $t \in T$.
\end{proposition}

We always assume that $T$ is a linearly ordered index set such as $\mathbb{Z}_{\ge 0}$ or $\mathbb{R}_{\ge 0}$.
\begin{definition}[filtration]
	Suppose $(\Omega,\mathcal{F},\mathrm{P})$ be a probability space. For every $t\in T$ let $\mathcal{F}_{t}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. Then
	\[
	\mathbb{F} =(\mathcal{F}_{t})_{t\in T}
	\]
	is called a \emph{filtration} on the probability space $(\Omega,\mathcal{F},\mathrm{P})$ if $\mathcal {F}_{k}\subset \mathcal{F}_{\ell}$ for all $k\leq \ell$. 
\end{definition}
\begin{definition}[filtered probability space]
	If $(\Omega,\mathcal{F},\mathrm{P})$ is a probability space and $ \mathbb {F}=(\mathcal{F}_{t})_{t\in T }$ is a filtration on the probability space $(\Omega,\mathcal{F},\mathrm{P})$, then $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ is called a \emph{filtered probability space}.
\end{definition}

\begin{definition}[adapted process]
	A stochastic process $X=(X_{t})_{t \in T}$ is called \emph{adapted} (to the filtration $(\mathcal{F}_{t})_{t\in T}$ ) if for any $t \in T, X_{t}$ is $\mathcal{F}_{t}$ -measurable, that is, $\sigma(X_t)\subset \mathcal{F}_t$ for all $t\in T$.
\end{definition}

A stochastic process $(X_{t})_{t\in T }$ is said to be defined on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ if $(X_{n})_{t\in T }$ is defined on $(\Omega,\mathcal{F},\mathrm{P})$ and adapted to the filtration $\mathbb{F} =(\mathcal{F}_{t})_{t\in T }$. 

The definition of $\mathcal{F}_{\infty}$ is usually specified as
\[
\mathcal{F}_{\infty}:=\sigma\left(\bigcup_{t\in T} \mathcal{F}_t\right)
\]

\begin{definition}[natural filtration]
	Let $X=(X_{t})_{t\ge 0}$ be a stochastic process on the probability space $(\Omega,\mathcal{F},\mathrm{P})$. Then 
	\[
	{\mathcal {F}}_{t}^X:=\sigma (\{X_{s}:0\le s\le t\})
	\]
	is a $\sigma$-algebra and $(\mathcal {F}_{t}^X)_{t\ge0} $ is a filtration that $X$ is adapted to. We call $(\mathcal {F}_{t}^X)_{t\ge0} $ the \emph{natural filtration} induced by the stochastic process $X$. $(\mathcal {F}_{t}^X)_{t\ge0} $ is the minimum filtration which $X$ is adapted to.
\end{definition}


\begin{definition}[right-continuous filtration]
	Given a filtration $\mathbb{F}=(\mathcal{F}_{t})_{t\in T}$, define
	\[
	\mathcal{F}_{t+}:=\bigcap_{s>t}\mathcal{F}_{s}\quad,\forall t\in T.
	\]
	Then $\mathbb{F}^+:=({\mathcal{F}}_{t+})_{t\in T}$ is a filtration. The filtration $\mathbb {F}$ is called \emph{right-continuous} if and only if $\mathbb{F}^{+}=\mathbb {F}$.
\end{definition}

\begin{definition}[complete filtration]
	Let
	\[\mathcal{N}^{\mathrm{P}}:=\{A\subset 2^\Omega | A\subset B\text{ for a }B\text{ with }\mathrm{P}(B)=0\}
	\]
	be the set of all sets that are contained within a $\mathrm{P}$-null set. A filtration $\mathbb{F} =(\mathcal{F}_{t})_{t\in T}$ is called a \emph{complete filtration}, if every $\mathcal{F}_{t}$ contains $\mathcal{N}^{\mathrm{P}}$. This is equivalent to $(\Omega,\mathcal{F}_{t},\mathrm{P})$ being a complete measure space for every $t\in T$. Let $\mathcal{F}_{t}^{\mathrm{P}}=\sigma\left(\mathcal{F}_{t} \cup \mathcal{N}^{\mathrm{P}}\right)$. Then $\mathbb{F}^{\mathrm{P}} =(\mathcal{F}_{t}^{\mathrm{P}})_{t\in T}$ is a complete filtration.
\end{definition}


\begin{definition}[usual conditions and stochastic basis]
	A filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ is called a \emph{stochastic basis} if the filtration $(\mathcal{F}_{t})_{t\in T }$ satisfies the following \emph{usual conditions}:
	\begin{enumerate}
		\item $(\mathcal{F}_{t})_{t\in T }$ is right-continuous;
		\item $(\mathcal{F}_{t})_{t\in T }$ is complete.
	\end{enumerate}
\end{definition}

\begin{definition}[measurable stochastic process]
	A stochastic process $X=(X_t)_{t\in T}$ defined on probability space $(\Omega,\mathcal{F},\mathrm{P})$ is \emph{measurable} if, for all $A \in \mathcal{B}\left(T\right)$,
	\[
	\left\{( \omega,t): X_{t}(\omega) \in A\right\} \in \mathcal{F}\otimes \mathcal{B}(T) .
	\]
\end{definition}

\begin{definition}[progressively measurable stochastic process]
	Let $X=(X_t)_{t\ge0}$ be a stochastic process defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$. If for all $M\ge0$, the mapping
	\begin{align*}
	X^{(M)}: \Omega \times [0, M]&\longrightarrow S\\
	(\omega,t)&\longmapsto X_t(\omega)
	\end{align*}
	is $\mathcal{F}_M\otimes \mathcal{B}([0,M])$ -measurable, we say $X$ is \emph{progressively measurable}.
\end{definition}



\begin{definition}[continuous (RCLL/right-continuous) stochastic process]
	A stochastic process $(X_t)_{t\ge0}$ defined on $(\Omega,\mathcal{F},\mathrm{P})$ with the state space $(\mathbb{R}^n,\mathcal{B}(\mathbb{R}^n))$ is continuous (RCLL\footnote{A RCLL ("right continuous with left limits"), càdlàg (French: "continue à droite, limite à gauche"), or corlol ("continuous on (the) right, limit on (the) left") function is a function defined on the real numbers (or a subset of them) that is everywhere right-continuous and has left limits everywhere.}/right-continuous) if there	is $\Omega_0\in\mathcal{F}$  with $P(\Omega_0) = 1$ such that the path $t\mapsto X_t(\omega)$ is continuous (RCLL/right-continuous) for every $\omega\in\Omega_0$.
\end{definition}




\begin{definition}[predictable process]
	Given a filtered probability space $\left(\Omega, \mathcal{F},(\mathcal{F}_{t})_{t \ge 0}, \mathrm{P}\right)$ with the state space $(\mathbb{R}^n,\mathcal{B}(\mathbb{R}^n))$, define the \emph{$(\mathcal{F}_{t})_{t \ge 0}$-predictable $\sigma$-algebra} as follows
	\[
	\sigma\left(\{X^{-1}(B)|X:\Omega\times[0,\infty)\longrightarrow \mathbb{R}^n\text{ is left-continuous adapted processes and } B\in\mathbb{R}^n\}\right).
	\]
	then the stochastic process $X=\left(X_{t}\right)_{t \ge 0}$ is \emph{predictable} if $X:\Omega\times[0,\infty)\longrightarrow \mathbb{R}^n$ is measurable with respect to the $(\mathcal{F}_{t})_{t \ge 0}$-predictable $\sigma$-algebra.
\end{definition}

\begin{proposition}
	Let $X=(X_t)_{t\ge0}$ be a stochastic process defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ with the state space $(\mathbb{R}^n,\mathcal{B}(\mathbb{R}^n))$. Then the implication relations of some properties of $X$ are shown as follows
	\begin{center}
	\begin{tikzcd}[arrows=Rightarrow]
		&\text{RCLL}\arrow{r}{}&\text{right-continuous}\arrow{rd}{}&&\\
		\text{continuous}\arrow{rd}{}\arrow{ru}{}&&&\text{progressively measurable}\arrow{r}{}&\text{measurable}\\
		&\text{left-continuous}\arrow{r}{}&\text{predictable}\arrow{ru}{}&&
	\end{tikzcd}
	\end{center}
\end{proposition}

\begin{definition}[modification and indistinguishability]
	Let $X, Y$ be stochastic processes from $\Omega\times T$ to $S$.
	$X$ is a \emph{modification} of $Y$ iff
	\[
	\forall t \in T,\; \mathrm{P}(X_t=Y_t)=1
	\]
	and $X$ is \emph{indistinguishable} from $Y$ iff
	\[
	\mathrm{P}(X=Y)=\mathrm{P}(\forall t \in T,\; X_t=Y_t)=1.
	\]
\end{definition}
If $X$ and $Y$ are indistinguishable, they are modifications of each other.
\begin{proposition}
	Let $X,Y$ be a process defined on the stochastic basis $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$. Then $X$ and $Y$ are indistinguishable iff
	they are modifications of each other.
\end{proposition}

\begin{proposition}
	Given any measurable process $X=(X_t)_{t\ge0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$, $X$ has a progressively measurable modification.
\end{proposition}



\begin{definition}[the distribution of a process]
	The distribution of a process $X: \Omega \rightarrow S^T$ is the pushforward measure $P\circ X^{-1}$ on $(S^T,\mathcal{S}^T)$.
\end{definition}
We use the notation $X \stackrel{d}{=} Y$ to represent that $X$ and $Y$ have the same distribution. If $X$ and $Y$ are modifications of each other, then $X \stackrel{d}{=} Y$.

\begin{definition}[family of finite dimensional distributions]
	The family 
	$$\mathfrak{D}_X:=\left\{\mu_{(t_{1}, t_{2}, \cdots, t_{k})}:= \mathrm{P}\circ\left(X_{t_{1}}, \cdots, X_{t_{k}}\right)^{-1}:\right.\left.\left(t_{1}, t_{2}, \cdots, t_{k}\right) \in T^k,k\ge1\right\}$$ of probability distributions is called	the \emph{family of finite dimensional distributions (fdds) associated with the stochastic process $(X_{t})_{t\in T}$}.
\end{definition}

\begin{proposition}
	Let $X,Y$ be processes on $(\Omega ,{\mathcal{F}},\mathrm{P})$ with paths in $S^T$. Then $X \stackrel{d}{=} Y$ iff
	\[
	\left(X_{t_{1}}, \cdots, X_{t_{n}}\right) \stackrel{d}{=}\left(Y_{t_{1}}, \cdots, Y_{t_{n}}\right), \quad \forall t_{1}, \cdots, t_{n} \in T,\quad n\ge 1.
	\]
\end{proposition}



\begin{proposition}[transfer of regularity]
		Let $S$ be a a separable metric space and $X, Y$ be processes on $(\Omega ,{\mathcal{F}},\mathrm{P})$ with paths in $U \subset S^T$ such that $X \stackrel{d}{=} Y$. Assume that $Y$ has paths in some set $U \subset S^{T}$ that is Borel for the $\sigma$-algebra $\mathcal{U}=(\mathcal{B}(S))^{T}\cap U$. Then even $X$ has a modification with paths in $U \subset S^{T}$.
\end{proposition}

\begin{definition}[independence of stochastic processes]
	$N$ stochastic processes $X^{(1)},X^{(2)},\cdots,X^{(N)}$ defined on the same probability space $(\Omega, \mathcal{F}, P)$ are said to be independent if for all $n\ge1$ and for all $t_{1}, \cdots, t_{n} \in T,$ the $N$ random vectors $\left(X^{(1)}_{t_{1}}, \cdots, X^{(1)}_{t_{n}}\right), \left(X^{(2)}_{t_{1}}, \cdots, X^{(2)}_{t_{n}}\right),\cdots,\left(X^{(N)}_{t_{1}}, \cdots, X^{(N)}_{t_{n}}\right)$ are independent, i.e. if
	\[
	F_{X^{(1)}_{t_{1}}, \cdots, X^{(1)}_{t_{n}}, \cdots, X^{(N)}_{t_{1}}, \cdots, X^{(N)}_{t_{n}}}\left(x^{(1)}_{1}, \cdots, x^{(1)}_{n}, \cdots, x^{(N)}_{1}, \cdots, x^{(N)}_{n}\right)=\prod_{i=1}^N F_{X^{(i)}_{t_1}, \cdots, X^{(i)}_{t_{n}}}\left(x^{(i)}_{1}, \cdots, x^{(i)}_{n}\right) .
	\]
\end{definition}

For simplicity, we always assume that $T=\mathbb{R}_{\ge 0}$ or $T=\mathbb{Z}_{\ge 0}$ and that $(S,\mathcal{S})=(\mathbb{R},\mathcal{B}(\mathbb{R}))$.

\begin{proposition}[consistency conditions]
	Given a family of finite dimensional distributions,
	$$
	\mathfrak{D}=\left\{\mu_{\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}\right)}:\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}\right) \in T^{k}, k\ge1 \right\},
	$$
	it satisfies the following consistency conditions: for any $k\ge2$, $\left(t_{1}, t_{2}, \cdots, t_{k}\right) \in T^k$, and any $B_{1}, B_{2}, \cdots, B_{k}$ in $\mathcal{B}(\mathbb{R})$,
	\begin{enumerate}
		\item[(C1)] $\mu_{\left(t_{1}, t_{2}, \cdots, t_{k}\right)}\left(B_{1} \times \cdots \times B_{k-1} \times \mathbb{R}\right)=\mu_{\left(t_{1}, t_{2}, \cdots, t_{k}\right)}\left(B_{1} \times \cdots \times B_{k-1}\right)$
		\item[(C2)] For any permutation $\left(i_{1}, i_{2}, \cdots, i_{k}\right)$ of $(1,2, \cdots, k)$,
		\[
		\mu_{\left(t_{i_{1}}, t_{i_{2}}, \cdots, t_{i_{k}}\right)}\left(B_{i_{1}} \times B_{i_{2}} \times \cdots \times B_{i_{k}}\right)=\mu_{\left(t_{1}, t_{2}, \cdots, t_{k}\right)}\left(B_{1} \times B_{2} \times \cdots \times B_{k}\right)
		\]
	\end{enumerate}
\end{proposition}

\begin{theorem}[Kolmogorov's consistency theorem]
	Let $T$ be a nonempty set. Let 
	$$
	\mathfrak{D}_{T} =\left\{\nu_{\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}\right)}:\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}\right) \in T^{k}, k\ge1 \right\}
	$$
	be a family of probability distributions such that for each $\left(t_{1}, t_{2}, \cdots, t_{k}\right) \in T^k,k\ge1$
	\begin{enumerate}[(i)]
		\item $\nu_{\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{k}\right)}$ is a probability distribution on $\left(\mathbb{R}^{k}, \mathcal{B}\left(\mathbb{R}^{k}\right)\right)$
		\item consistency conditions C1 and C2 hold
	\end{enumerate}
	Then, there exists a probability space $(\Omega, \mathcal{F}, P)$ and a stochastic process $(X_t)_{t\in T}$ on $(\Omega, \mathcal{F}, P)$ such that $D_{T}$ is the family of finite dimensional distributions associated with $(X_t)_{t\in T}$.
\end{theorem}





\begin{definition}[strictly stationary process]
	Let $(X_t)_{t\in T}$ be a stochastic process and let \newline $F_{{X}}(x_{{t_{1}+\tau }},\cdots ,x_{{t_{k}+\tau }})$ represent the  distribution function of the joint distribution of $(X_t)_{t\in T}$ at times $t_{1}+\tau ,\cdots ,t_{k}+\tau$ . Then, $(X_t)_{t\in T}$ is said to be strictly stationary if, for all $k$, for all $\tau$, and for all $ t_{1},\cdots ,t_{k}$,
	\[
	F_{{X}}(x_{{t_{1}+\tau }},\cdots ,x_{{t_{k}+\tau }})=F_{{X}}(x_{{t_{1}}},\cdots ,x_{{t_{k}}}).
	\]
\end{definition}

\begin{definition}[independent increments]
	A stochastic process $(X_t)_{t\in T}$ has \emph{independent increments} if for every $n\in \mathbb{N}_+$ and any $t_1\le t_2 \le\cdots\le t_n$, the increments $X_{t_2}-X_{t_1},X_{t_3}-X_{t_2},\cdots,X_{t_n}-X_{t_{n-1}}$ are independent.
\end{definition}

\begin{definition}[stationary increments]
	A stochastic process $(X_t)_{t\in T}$ has \emph{stationary increments} if for all $s<t$, the probability distribution of the increments $X_{t}-X_{s}$ depends only on $t-s$.
\end{definition}

\chapter{Poisson Process}
\section{Poisson Process}
\begin{definition}[Poisson process (I)]
	A stochastic process $(N_t)_{t\ge0}$ defined on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ is said to be a \emph{Poisson process} with rate $\lambda>0$ if
	\begin{enumerate}[(i)]
		\item $N_0=0$;
		\item \hypertarget{Definition 2.1(ii)}{} $(N_t)_{t\ge0}$ has independent increments: for any $n\in \mathbb{N}_+$ and any $0\le t_1\le t_2 \le\cdots\le t_n$, the increment $N_{t_2}-N_{t_1},N_{t_3}-N_{t_2},\cdots,N_{t_n}-N_{t_{n-1}}$ are independent;
		\item \hypertarget{Definition 2.1(iii)}{}for any $0\le s < t$, $N_t-N_s\sim Pois(\lambda(t-s))$, that is 
		\[
		\mathrm{P}(N_t-N_s=k)=e^{-\lambda(t-s)}\dfrac{\lambda(t-s)^k}{k!}\quad(k=0,1,2,\cdots).
		\]
	\end{enumerate}	
\end{definition}

\begin{definition}[counting process]
	A \emph{counting process} is a stochastic process $(N_t)_{t\ge 0}$ with values that are non-negative, integer, and non-decreasing:
	\begin{enumerate}[(i)]
		\item $N_t\ge0$;
		\item $N_t$ is an integer;
		\item If $0\le s\le t$, then $N_s \le N_t$.
	\end{enumerate}	
		
\end{definition}
For any $0\le s<t$, the counting process $N_t-N_s$ represents the number of events that occurred on $(s,t]$.  


\begin{definition}[Poisson process (II)]
	A counting process $(N_t)_{t\ge0}$ defined on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ is said to be a \emph{Poisson process} with rate $\lambda>0$ if
	\begin{enumerate}[(i)]
		\item $N_0=0$;
		\item $(N_t)_{t\ge0}$ has independent increments;
		\item \hypertarget{Definition 2.3(iii)}{}For all $t\ge0$, $\mathrm{P}(N_{t+h}-N_t=1)=\lambda h+o(h)$ when $h\to0$;
		\item \hypertarget{Definition 2.3(iv)}{}For all $t\ge0$, $\mathrm{P}(N_{t+h}-N_t\ge2)=o(h)$ when $h\to0$;
	\end{enumerate}	
\end{definition}

\begin{definition}[Poisson process (III)]
	A stochastic process $(N_t)_{t\ge0}$ defined on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ is said to be a \emph{Poisson process} with rate $\lambda>0$ if
	\[
	N_t=\sum_{n=1}^{\infty}nI_{[T_n,T_{n+1})}(t),
	\]
	where $T_n=X_1+X_2+\cdots+X_n$ and $X_i\text{ i.i.d }\sim Exp(\lambda)$ (Here the pdf of $Exp(\lambda)$ is taken as $\lambda e^{-\lambda x}I_{(0,+\infty)}(x)$).
\end{definition}

\begin{proposition}
	Definition 2.1, 2.3, 2.4 are equivalent definitions of Poisson process.
\end{proposition}
\begin{proof}~\\
	\vspace{-1em}
	\begin{itemize}
	\item \underline{Definition 2.1 $\implies$ Definition 2.3} 
	
	Here we are only to show the implication of \hyperlink{Definition 2.3(iii)}{Definition 2.3(iii)} and \hyperlink{Definition 2.3(iv)}{Definition 2.3(iv)}. Since $N_{t+h}-N_t\sim Pois(\lambda h)$, when $h\to0$ we have 
	\begin{align*}
		\mathrm{P}(N_{t+h}-N_t=1)&=e^{-\lambda h}\lambda h=(1-\lambda h+o(h))\lambda h=\lambda h+o(h),\\
		\mathrm{P}(N_{t+h}-N_t\ge2)&=1-\mathrm{P}(N_{t+h}-N_t=0)-\mathrm{P}(N_{t+h}-N_t=1)\\
		&=1-e^{-\lambda h}-e^{-\lambda h}\lambda h\\
		&=1-(1-\lambda h+o(h))-(\lambda h+o(h))\\
		&=o(h).
	\end{align*}
	\item\underline{Definition 2.3 $\implies$ Definition 2.1} 
	
	Only \hyperlink{Definition 2.1(iii)}{Definition 2.1(iii)} needs to be derived. Given the Laplace transform of the nonnegative random variables $N_t$ and $N_{t+h}$
	\[
	L_{N_t}(u)=\mathrm{E}[e^{-uN_t}],\quad
	L_{N_{t+h}}(u)=\mathrm{E}[e^{-uN_{t+h}}],\quad u\ge0,
	\]
	according to Definition 2.3(ii) we can obtain
	\begin{align*}
	L_{N_{t+h}}(u)&=\mathrm{E}[e^{-uN_{t+h}}]\\
	&=\mathrm{E}[e^{-uN_{t}}e^{-u(N_{t+h}-N_t)}]\\
	&=\mathrm{E}[e^{-uN_{t}}]\mathrm{E}[e^{-u(N_{t+h}-N_t)}]\\
	&=L_{N_t}(u)\mathrm{E}[e^{-u(N_{t+h}-N_t)}].
	\end{align*}
	Note that 
	\begin{align*}
	&\mathrm{E}[e^{-u(N_{t+h}-N_t)}]\\
	=\ &e^{0}\mathrm{P}(N_{t+h}-N_t=0)+e^{-u}\mathrm{P}(N_{t+h}-N_t=1)+\sum_{j=2}^{\infty}e^{-un}\mathrm{P}(N_{t+h}-N_t=j)\\
	=\ &1-\lambda h+o(h)+e^{-u}(\lambda h+o(h))+o(h)\\
	=\ &1-\lambda h+e^{-u}\lambda h+o(h)\quad(h\to 0).
	\end{align*}
	Denote $g(t+h)=L_{N_{t+h}}(u)$ and $g(t)=L_{N_{t}}(u)$ for some fixed $u$ and then we get 
	\[
	\frac{g(t+h)-g(t)}{h}=\frac{g(t)(1-\lambda h+e^{-u}\lambda h+o(h))-g(t)}{h}=g(t)\lambda (e^{-u}-1)+\frac{o(h)}{h}.
	\]
	Letting $h\to 0$ yields the differential equation
	\[
	g'(t)=g(t)\lambda (e^{-u}-1).
	\]
	The initial condition $g(0)=\mathrm{E}[e^{-uN_{0}}]=1$ determines a special solution of the equation 
	$$g(t)=L_{N_{t}}(u)=e^{\lambda t (e^{-u}-1)},$$
	which coincides with the Laplace transform of Poisson distribution $Pois(\lambda t)$. Since Laplace transform uniquely determines the distribution, we can thus conclude $N_{t}\sim Pois(\lambda t)$. Given any $r\ge 0$, define a stochastic process $N'_t=N_{r+t}-N_r$ and we can check that $(N'_t)_{t\ge 0}$ is also a counting process satisfying all the contitions in Definition 2.3. Hence by repeating the proof above we can show $N'_{t}\sim Pois(\lambda t)$, which is equivalent to Definition 2.1(iii).

	\item\noindent\underline{Definition 2.1 $\implies$ Definition 2.4}\\
	Let $T_n=\inf\{t\ge0:N_t= n\}$ for $n\in\mathbb{N}_+$. Note that given any $t\ge0$, $\{N_t=n\}=\{T_n\le t<T_{n+1}\}$. Thus we have
	\[
	N_t=\sum_{n=1}^{\infty}nI_{N_t=n}=\sum_{n=1}^{\infty}nI_{[T_n,T_{n+1})}(t).
	\]
	Let $X_1=T_1,X_{n}=T_{n}-T_{n-1}(n\ge2)$. Since $\mathrm{P}(X_1>t)=\mathrm{P}(N_t=0)=e^{-\lambda t}$, we see $X_1\sim Exp(\lambda)$. 
	Since
	\[
	\mathrm{P}(X_2>t|X_1=t_1)=\mathrm{P}(X_2>t|X_1=t_1)
	\]
	When $n\ge2$, since
	\begin{align*}
	&\mathrm{P}(X_n>t|X_{n-1}=t_{n-1},\cdots,X_1=t_1)\\
	=\;&\mathrm{P}(T_{n}-T_{n-1}>t|T_{n-1}-T_{n-2}=t_{n-1},\cdots,T_1=t_1)\qquad(\text{let }s_n=t_{n}+\cdots +t_1)\\
	=\;&\mathrm{P}(T_{n}>s_{n-1}+t|T_{n-1}=s_{n-1},\cdots,T_1=s_1)\\
	=\;&\mathrm{P}(N_{s_{n-1}+t}=n-1|N_{s_{n-1}}=n-1)\qquad(\text{memoryless property of }(N_t))\\
	=\;&\mathrm{P}(N_{s_{n-1}+t}-N_{s_{n-1}}=0|N_s=n-1)\\
	=\;&\mathrm{P}(N_{s_{n-1}+t}-N_{s_{n-1}}=0)\\
	=\;&e^{-\lambda t},
	\end{align*}
	it is plain to show that $\{X_i\}$ is sequence of independent random variable. Furthermore, we have $$\mathrm{P}(X_n>t)=\mathrm{E}[\mathrm{P}(X_n>t|X_{n-1},\cdots,X_1)]=e^{-\lambda t},$$
	which implies $X_i\text{ i.i.d }\sim Exp(\lambda)$,
	\item\noindent\underline{Definition 2.4 $\implies$ Definition 2.1}
	
	\noindent Clearly $N_0=0$ holds. Since $T_n=X_1+X_2+\cdots+X_n$ and $X_i\text{ i.i.d }\sim Exp(\lambda)$, we can deduce the jointly probability density function of $(T_1,T_2,\cdots,T_m)$ 
	\begin{align*}
	f_S(y_1,y_2,\cdots,y_m)&=f_X(y_1,y_2-y_1,\cdots,y_m-y_{m-1})\left|\frac{\partial(x_1,\cdots,x_m)}{\partial(y_1,\cdots,y_m)}\right|\\
	&=\lambda^m e^{-\lambda y_m}I_{\{0\le y_1<\cdots<y_m\}}.
	\end{align*}
	Thus for any $1\le j_1<j_2<\cdots<j_n$, the jointly probability density function of $(T_{j_1},T_{j_2},\cdots,T_{j_n})$ is
	\[
	\frac{y_1^{j_1-1}}{(j_1-1)!}\frac{(y_2-y_1)^{j_2-j_1-1}}{(j_2-j_1-1)!}\cdots\frac{(y_n-y_{n-1})^{j_n-j_{n-1}-1}}{(j_n-j_{n-1}-1)!}\lambda^{j_n}e^{-\lambda y_n}I_{\{0\le y_1<\cdots<y_n\}}
	\]
	Note $$N_t=\sum_{n=1}^{\infty}nI_{[T_n,T_{n+1})}(t)$$ implies $\{N_t=n\}=\{T_n\le t<T_{n+1}\}$. For any $n\in \mathbb{N}_+$ and any $0\le t_1<t_2<\cdots<t_n$, we have
	\begin{align*}
	&\ \mathrm{P}(N_{t_1}=j_1,N_{t_1}-N_{t_2}=j_2,\cdots,N_{t_{n}}-N_{t_{n-1}}=j_n)\\
	=&\ 
	\mathrm{P}(N_{t_1}=j_1,N_{t_1}=j_1+j_2,\cdots,N_{t_{n}}=j_1+\cdots+j_n)\quad(\text{let }k_n=j_1+\cdots j_n)\\
	=&\ \mathrm{P}(T_{k_1}\le t_1,T_{k_1+1}>t_1,T_{k_2}\le t_2,T_{k_2+1}>t_2,\cdots,T_{k_n}\le t_n,T_{k_n+1}>t_n)\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<y_{k_n}\le t_n<y_{k_n+1}} \frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_n}-y_{k_{n-1}+1})^{k_n-k_{n-1}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n+1}e^{-\lambda y_{k_n+1}}dy_1\cdots dy_{k_n+1}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<y_{k_n}\le t_n} \frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_n}-y_{k_{n-1}+1})^{k_n-k_{n-1}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}dy_{k_1}\cdots dy_{k_n}\int_{t_n}^{\infty}-de^{-\lambda y_{k_n+1}}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<t_{n-1}<y_{k_{n-1}+1}< t_n}\\
	&\frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_{n-1}}-y_{k_{n-2}+1})^{k_{n-1}-k_{n-2}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}e^{-\lambda t_n}dy_1\cdots dy_{k_{n-1}+1}\int_{y_{k_{n-1}+1}}^{t_n}d\frac{(y_{k_n}-y_{k_{n-1}+1})^{k_n-k_{n-1}-1}}{(k_n-k_{n-1}-1)!}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<t_{n-1}<y_{k_{n-1}+1}<t_n}\\
	&\frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_{n-1}}-y_{k_{n-2}+1})^{k_{n-1}-k_{n-2}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}e^{-\lambda t_n}dy_1\cdots dy_{k_{n-1}}\int_{t_{n-1}}^{t_n}-d\frac{(t_{n}-y_{k_{n-1}})^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<t_{n-1}<y_{k_{n-1}+1}<t_n}\\
	&\frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_{n-1}}-y_{k_{n-2}+1})^{k_{n-1}-k_{n-2}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}e^{-\lambda t_n}dy_1\cdots dy_{k_{n-1}}\frac{(t_{n}-t_{n-1})^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}\\
	=&\cdots\\
	=&\lambda^{k_n}e^{-\lambda t_n}\frac{t_{1}^{k_1}}{k_1!}\frac{(t_{2}-t_{1})^{k_2-k_{1}}}{(k_2-k_{1})!}\cdots\frac{(t_{n}-t_{n-1})^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}\\
	=&e^{-\lambda t_1}\frac{(\lambda t_1)^{j_1}}{j_1}e^{-\lambda(t_2- t_1)}\frac{(\lambda (t_2-t_1))^{j_2}}{j_2}\cdots e^{-\lambda(t_n-t_{n-1}) }\frac{(\lambda(t_{n}-t_{n-1}))^{j_n}}{j_n!}
\end{align*}
	Therefore, we conclude $N_{t_2}-N_{t_1},N_{t_3}-N_{t_2},\cdots,N_{t_n}-N_{t_{n-1}}$ are independent and for any $0\le s < t$, $N_t-N_s\sim Pois(\lambda(t-s))$.
\end{itemize}
\end{proof}

\begin{proposition}
	Let $(N_t)_{t\ge0}$ be a Poisson process.
	\begin{enumerate}
		\item $N_t\sim Pois(\lambda t)$, $\mathrm{E}[N_t]=\mathrm{Var}(N_t)=\lambda t$.
		\item For $0\le s\le t$, $\mathrm{E}[N_tN_s]=\lambda^2ts+\lambda s$, $\mathrm{Cov}(E_t,E_s)=\lambda s$.
		\item For $0\le s\le t$, $\mathrm{E}[N_t|N_s]=N_s+\lambda(t-s)$. So Poisson process is a submartingale.
		\item Poisson process is a Markov process. For $0\le t_1< t_2<\cdots<t_n$ and $0\le k_1\le k_2\le\cdots\le k_n$,
		\begin{align*} &\mathrm{P}(N_{t_n}=k_n|N_{t_{n-1}}=k_{n-1},\cdots,N_{t_1}=k_1)\\
		=\ &\mathrm{P}(N_{t_n}=k_n|N_{t_{n-1}}=k_{n-1})\\
		=\ &\mathrm{P}(N_{t_n}-N_{t_{n-1}}=k_n-k_{n-1})\\
		=\ &e^{-\lambda(t_n-t_{n-1}) }\frac{(\lambda(t_{n}-t_{n-1}))^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}. 
		\end{align*}
	\end{enumerate}
\end{proposition}
\begin{proof}
	Apply \hyperlink{Definition 2.1(ii)}{Definition 2.1(ii)} and it is straightforward to show the properties.
\end{proof}

\section{Compound Poisson Process}
\begin{definition}[compound Poisson distribution]
	Suppose that
	$N\sim Pois(\lambda )$ and that $Z_{1},Z_{2},Z_{3},\cdots $ are i.i.d. random variables independent of $N$ with a probability measure $v(dy)$ on $\mathbb{R}$. Then the probability distribution of the sum of $N$ i.i.d. random variables
	\[
	Y=\sum _{n=1}^{N}Z_{n}
	\]
	is a \emph{compound Poisson distribution}.
\end{definition}


\begin{definition}[compound Poisson process]
	\emph{A compound Poisson process}, parameterised by a rate $\lambda >0$ and jump size distribution $v(dy)$, is a process $(Y_t)_{t\ge 0}$ given by
	\[
	Y_t=\sum _{n=1}^{N_t}Z_{n},
	\]
	where $(N_t)_{t\ge 0}$ is a Poisson process with rate $\lambda$, and $(Z_n)_{n\in \mathbb{N}_+}$ are independent and identically distributed random variables with distribution $v(dy)$, which are also independent of $(N_t)_{t\ge 0}$.
\end{definition}

\begin{proposition}
	Let $(Y_t)_{t\ge 0}$ be a compound Poisson process with a rate $\lambda$ and jump $(Z_n)_{n\in \mathbb{N}_+}$. For convenience, assume $Z_n\overset{d}{=}Z$ and $\mathrm{E}[Z^2]<+\infty$.
	\begin{enumerate}
		\item $\mathrm{E}[Y_t]=\lambda t\mathrm{E}[Z]$.
		\item $\mathrm{Var}(Y_t)=\lambda t\mathrm{E}[Z^2]$.
		\item The moment generating function $M_{Y_t}(a)=\mathrm{E}[e^{aY_t}]=e^{\lambda t(\mathrm{E}[e^{aZ}]-1)}=e^{\lambda t(M_Z(a)-1)}$
	\end{enumerate}
\end{proposition}
\begin{proof}\hspace*{1em}	
\begin{enumerate}
	\item Since $Z_n$ is independent of $N_t$, we have $$\mathrm{E}[Y_t]=\mathrm{E}[\mathrm{E}[Y_t|N_t]]=\mathrm{E}\left[\mathrm{E}\left[\left.\sum _{n=1}^{N_t}Z_{n}\right|N_t\right]\right]=\mathrm{E}\left[\sum _{n=1}^{N_t}\mathrm{E}\left[\left.Z_{n}\right|N_t\right]\right]=\mathrm{E}[N_tZ]=\mathrm{E}[N_t]\mathrm{E}[Z]=\lambda t\mathrm{E}[Z].$$
	\item Since $Z_n$ is independent of $N_t$, by the law of total variance $\mathrm{Var}(Y_t)$ can be calculated as
	\begin{align*}
		\mathrm{Var}(Y_t)&=\mathrm{E}[\mathrm{Var}(Y_t|N_t)]+\mathrm{Var}(\mathrm{E}[Y_t|N_t])\\
		&=\mathrm{E}[N_t\mathrm{Var}(Z)]+\mathrm{Var}(N_t\mathrm{E}[Z])\\
		&=\mathrm{Var}(Z)\mathrm{E}[N_t]+\mathrm{E}[Z]^2\mathrm{Var}(N_t)\\
		&=\lambda t\mathrm{Var}(Z)+\lambda t\mathrm{E}[Z]^2\\
		&=\lambda t \mathrm{E}[Z^2].
	\end{align*}
	\item Make similar use of the dependence of $(Z_n)_{n\in\mathbb{N_+}}$ and $N_t$ to get
	\[
	\begin{aligned}
	\mathrm{E}\left[e^{aY_t}\right]&=\mathrm{E}\left[e^{a\left(Z_1+Z_2+\cdots+Z_{N_t}\right)}\right]\\
	&=\mathrm{E}\left[\mathrm{E}\left[\left.e^{a\left(Z_1+Z_2+\cdots+Z_{N_t}\right)}\right|N_t\right]\right]\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[\left.e^{a\left(Z_1+Z_2+\cdots+Z_{N_t}\right)}\right|N_t=n\right]\mathrm{P}\left(N_t=n\right)\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[\left.e^{a\left(Z_1+Z_2+\cdots+Z_{n}\right)}\right|N_t=n\right]e^{-\lambda t}\frac{\left(\lambda t\right)^n}{n!}\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[e^{aZ_1}e^{aZ_2}\cdots e^{aZ_{n}}\right]e^{-\lambda t}\frac{\left(\lambda t\right)^n}{n!}\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[e^{aZ}\right]^ne^{-\lambda t}\frac{\left(\lambda t\right)^n}{n!}\\
	&=e^{-\lambda t}\sum_{n=0}^{\infty}\frac{\left(\lambda t\mathrm{E}\left[e^{aZ}\right]\right)^n}{n!}\\
	&=e^{\lambda t\left(\mathrm{E}\left[e^{aZ}\right]-1\right)}.
 	\end{aligned}
 	\]
\end{enumerate}
\end{proof}
Let $Y=(Y_t)_{t\ge 0}$ be a compound Poisson process with a rate $\lambda$ and jump $(Z_n)_{n\in \mathbb{N}_+}$. Let $T_n=\inf\{t\ge0:N_t= n\}$ be the time when the $n$th event happens. Then the Itô integral of a stochastic process $K$ with respect to $Y$ is
\[
\int_{0}^{t}KdY=\sum_{n=0}^{N_t}K_{T_n}(Y_{T_n}-Y_{T_n-})=\sum_{n=0}^{N_t}K_{T_n}Z_n.
\]


\chapter{Markov Chain}
\section{Discrete-time Markov Chain}
\begin{definition}[discrete-time Markov chain]
	A \emph{discrete-time Markov chain} on a countable state space $S$ is a sequence of random variables $X_0, X_1, X_2,\cdots$ with the Markov property, namely that $\forall n\ge0,\ \forall j,i_0,i_1,\cdots,i_n\in S$,	
	\[
	\mathrm{P}(X_{n+1}=j| X_{0}=i_{0},X_{1}=i_{1},\cdots,X_{n}=i_{n})=\mathrm{P}(X_{n+1}=j| X_{n}=i_{n}), 
	\] if both conditional probabilities are well defined, i.e., if $ 	\mathrm{P}(X_{0}=i_{0},\cdots ,X_{n}=i_{n})>0$.
\end{definition}	
Markov property may be interpreted as stating that the probability of moving to the next state depends only on the present state and not on the previous states. 

\noindent A discrete-time Markov chain $(X_n)_{n\ge0}$ is \emph{time-homogeneous} if
\[
\mathrm{P}(X_{n+2}=j| X_{n+1}=i)=\mathrm{P}(X_{n+1}=j| X_{n}=i)
\]
for all $n\ge0$ and all $i,j\in S$. We only focus on the discrete-time time-homogeneous Markov chain on a countable space if nothing is specified. 

\noindent One can consider the (possibly infinite-dimensional) \emph{transition matrix}	$P=(p_{ij})_{i,j\in S}$, where
\[
p_{ij}=\mathrm{P}(X_{n+1}=j| X_{n}=i)
\]
is called \emph{one-step transition probability}. By the law of total probability it is clear to see 
\[
\sum_{j\in S}p_{ij}=1,\quad\forall i\in S.
\]
Similarly we can define \emph{$n$-step transition matrix} $P^{(n)}=\left(p^{(n)}_{ij}\right)_{i,j\in S}$, where \emph{$n$-step transition probabilities} $p^{(n)}_{ij}$ is the probability that a process in state $i$ will be in state $j$ after $n$ additional transitions. That is,
\[
p^{(n)}_{ij}=\mathrm{P}(X_{k+n}=j| X_{k}=i).
\]
As is shown below, the Chapman–Kolmogorov equation enables us to calculate $n$-step transition matrix readily.
\begin{proposition}[Chapman–Kolmogorov equation]\hypertarget{Proposition 4.1}{}
Let $(X_n)_{n\ge0}$ be a discrete-time Markov chain on a countable state space $S$. The \emph{Chapman–Kolmogorov equation} states that
\[
p^{(n+m)}_{ij}=\sum_{k\in S}p^{(n)}_{ik}p^{(m)}_{kj},\quad\forall i,j\in S,
\]
or alternatively in matrix form
\[
P^{(n+m)}=P^{(n)}P^{(m)}.
\]
\end{proposition}
\begin{proof}
	\[
	\begin{aligned}
		p^{(n+m)}_{ij}&=\mathrm{P}(X_{n+m}=j| X_{0}=i)\\
		&=\sum_{k\in S}\mathrm{P}(X_{n+m}=j,X_{n}=k| X_{0}=i)\\
		&=\sum_{k\in S}\mathrm{P}(X_{n}=k| X_{0}=i)\mathrm{P}(X_{n+m}=j| X_{0}=i,X_{n}=k)\\
		&=\sum_{k\in S}\mathrm{P}(X_{n}=k| X_{0}=i)\mathrm{P}(X_{n+m}=j| X_{n}=k)\\
		&=\sum_{k\in S}p^{(n)}_{ik}p^{(m)}_{kj}
	\end{aligned}
	\]
\end{proof}
\noindent Of course $P^{(1)}=P$. Thus by iteration we show $P^{(n)}$ coincides with $P^n$. 

\noindent Let $\pi(n)=\left(p_i^{(n)}\right)_{i\in S}$ denote the probability distribution of $X_n$, where $p_i^{(n)}=\mathrm{P}(X_n=i)$. Then we have
\[
\pi(n)=\pi(0)P^n.
\]

\section{Continuous-time Markov Chain}

\begin{definition}[continuous-time Markov chain]
	A \emph{continuous-time Markov chain} on a countable state space $S$ is a stochastic $(X_t)_{t\ge 0}$ with the Markov property: for all $ n\ge0$, all $0\le t_0\le t_1\le \cdots\le t_n$, and all	$j,i_0,\cdots,i_n\in S$,
	\[
	\mathrm{P}(X_{t_{n+1}}=j| X_{t_0}=i_{1},X_{t_1}=i_{2},\cdots,X_{t_n}=i_{n})=\mathrm{P}(X_{t_{n+1}}=j| X_{t_n}=i_{n}), 
	\] if both conditional probabilities are well defined, i.e., if $ 	\mathrm{P}(X_{1}=i_{1},\cdots ,X_{n}=i_{n})>0$.
\end{definition}

\noindent A continuous-time Markov chain $(X_t)_{t\ge0}$ is \emph{time-homogeneous} if
\[
\mathrm{P}(X_{s+t}=j| X_{s}=i)=\mathrm{P}(X_{t}=j| X_{0}=i)=p_{ij}(t)
\]
for all $s,t\ge0$ and all $i,j\in S$. 
\begin{example}
	Poisson progress $(N_t)_{t\ge0}$ is continuous-time time-homogeneous Markov chain.
\end{example}
We only focus on the continuous-time time-homogeneous Markov chain on a countable space if nothing is specified.
In this case, transition probability $p_{ij}(t)$ depends on the interarrival $t$ from state $i$ to state $j$.
\noindent One can define the \emph{transition matrix}	$P(t)=(p_{ij}(t))_{i,j\in S}$ and we have the similar properties of $p_{ij}(t)$ like the concrete-time case.
\begin{proposition}
	Let $(X_t)_{t\ge0}$ be a continuous-time Markov chain on a countable state space $S$. 
	\begin{enumerate}
		\item For all $t\ge 0$ and all $j\in S$,
		\[
		\sum_{j\in S}p_{ij}(t)=1.
		\]
		\item The \emph{Chapman–Kolmogorov equation} states that
		\[
		p_{ij}(t+s)=\sum_{k\in S}p_{ik}(t)p_{kj}(s),\quad\forall i,j\in S,
		\]
		or alternatively in matrix form
		\[
		P(t+s)=P(t)P(s).
		\]
	\end{enumerate}
\end{proposition}
\begin{proof}\hspace{1em}
	\begin{enumerate}
	\item It follows by the law of total probability.
	\item Imitate the proof in \hyperlink{Proposition 4.1}{Proposition 4.1} and the result is straightforward.
	\end{enumerate} 
\end{proof}
\begin{definition}
A continuous-time Markov chain is \emph{regular} if it satisfy the following condition
\[
\lim_{t\to0}\;p_{ij}(t)=\delta_{ij}=
\begin{cases}
1,&i=j,\\
0,&i\ne j.
\end{cases}
\]
\end{definition}
Since $p_{ij}(0)=\mathrm{P}(X_{t}=j| X_{0}=i)=\delta_{ij}$, regularity implies $p_{ij}(t)$ is continuous at $t=0$. Furthermore, we have the following stronger results. We always assume continuous-time Markov chain is regular afterwards.
\begin{lemma}
	If a continuous-time Markov chain is regular, for any fixed $i,j\in S$, $p_{ij}(t)$ is uniformly continuous with respect to $t$. 
\end{lemma}
\begin{proof}
	Since when $h>0$
	\[
	\begin{aligned}
		p_{ij}(t+h)-p_{ij}(t)&=\sum_{k\in S}p_{ik}(h)p_{kj}(t)-p_{ij}(t)\\
		&= p_{ii}(h)p_{ij}(t)-p_{ij}(t)+\sum_{k\in S\setminus\{i\}}p_{ik}(h)p_{kj}(t)\\
		&= -(1-p_{ii}(h))p_{ij}(t)+\sum_{k\in S\setminus\{i\}}p_{ik}(h)p_{kj}(t),
	\end{aligned}
	\]
	we have
	\[
	\begin{aligned}
	&p_{ij}(t+h)-p_{ij}(t)\ge -(1-p_{ii}(h))p_{ij}(t)\ge-(1-p_{ii}(h)),\\
	&p_{ij}(t+h)-p_{ij}(t)\le\sum_{k\in S\setminus\{i\}} p_{ik}(h)p_{kj}(t)\le \sum_{k\in S\setminus\{i\}}p_{ik}(h)=1-p_{ii}(h),	
	\end{aligned}
	\]
	which implies
	\[
	|p_{ij}(t+h)-p_{ij}(t)|\le 1-p_{ii}(h).
	\]
	When $h<0$ in a similar way we can get
	\[
	|p_{ij}(t)-p_{ij}(t+h)|\le 1-p_{ii}(-h).
	\]
	In general
	\[
	|p_{ij}(t+h)-p_{ij}(t)|\le 1-p_{ii}(|h|).
	\]
	According to regular condition we conclude for any $t\ge0$,
	\[
	\lim_{h\to0}|p_{ij}(t+h)-p_{ij}(t)|=0,
	\]
	that is, $p_{ij}(t)$ is uniformly continuous with respect to $t$ on $[0,\infty)$.
\end{proof}
If $p_{ij}(t)$ is differentiable, define the \emph{transition rate} 
\[
q_{ij}=\left.\frac{dp_{ij}(t)}{dt}\right|_{t=0}=\lim\limits_{h\to 0}\frac{p_{ij}(h)-p_{ij}(0)}{h}.
\]
The $q_{ij}$ can be seen as measuring how quickly the transition from $i$ to $j$ happens. Then define the \emph{transition rate matrix} $Q=(q_{ij})_{i,j\in S}$ with dimensions equal to that of the state space. 

\noindent Since $P(0)=I$, it can be shown that
\[
\mathrm{P}(X_{t+h}=j| X_t=i)=p_{ij}(h)=\delta _{ij}+q_{ij}h+o(h). 
\]
As an intermediate consequence, if the state space $S$ is finite, we have   
\[
\sum_{j\in S}q_{ij}=0,
\]
which follows by
\[
1=\sum_{j\in S}p_{ij}(h)=\sum_{j\in S}[\delta _{ij}+q_{ij}h+o(h)]=1+\sum_{j\in S}q_{ij}h+o(h).
\]
\begin{theorem}
	Let $(X_t)_{t\ge0}$ be a continuous-time Markov chain on a countable state space $S$.
	\begin{enumerate}
		\item Kolmogorov forward equation:
		\[
		p_{ij}'(t)=\sum_{k\in S}p_{ik}(t)q_{kj}
		\]
		or alternatively
		\[
		P'(t)=P(t)Q(t).
		\]
		\item Kolmogorov backward equation:
		\[
		p_{ij}'(t)=\sum_{k\in S}q_{ik}p_{kj}(t)
		\]
		or alternatively
		\[
		P'(t)=Q(t)P(t).
		\]
	\end{enumerate}
\end{theorem}

\chapter{Brownian Motion}
\section{1-dimensional Brownian Motion}
\begin{definition}[Brownian motion]
	A stochastic process $(B_t)_{t\ge0}$ is called a \emph{Brownian motion} if
	\begin{enumerate}
		\item $B_0=0$ a.s.
		\item $(B_t)_{t\ge0}$ has continuous path, that is $t\mapsto B_t$ is almost surely continuous.
		\item $(B_t)_{t\ge0}$ has independent and stationary increments.
		\item For $t>0$, $B_t\sim N(0,t)$.
	\end{enumerate}
\end{definition}

\begin{definition}[Gaussian process]
	A stochastic process $(X_t)_{t\in T}$ is a \emph{Gaussian process} if and only if for every finite set of indices $t_{1},\cdots ,t_{n}$ in the index set $T$, $(X_{t_1},X_{t_2},\cdots,X_{t_n})$ follows multivariate normal distribution $N(\mu,\Sigma)$.
\end{definition}

\begin{theorem}
	$B=(B_t)_{t\ge0}$ is a Brownian motion if and only if $B$ is a Gaussian process satisfying
	\begin{enumerate}
		\item $B_0=0$,
		\item $B$ has continuous paths,
		\item For all $t\ge0$, $\mathrm{E}[B_t]=0$,
		\item For all $s,t\ge0$, $\mathrm{E}[B_sB_t]=s\wedge t$.
	\end{enumerate}
\end{theorem}
\begin{proposition}
	Let $B=(B_t)_{t\ge0}$ be a Brownian motion.
	\begin{enumerate}
		\item For $k\ge1$, $\mathrm{E}[B_t^{2k-1}]=0$, $\mathrm{E}[B_t^{2k}]=t^{k}(2k-1)!!$.
		\item $B$ is a Markov process. 
		\item $B$ is a martingale.
	\end{enumerate}
\end{proposition}

\begin{theorem}
	The quadratic variation of a Brownian motion $B$ exists, and is given by $\langle B\rangle_{t}= t$. 
\end{theorem}
\begin{proof} 
	Given a partition $P$ of the interval $[0,t]$, we have
	\[
	\begin{aligned}
	\mathrm{E}\left[\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}\right]&=\sum _{k=1}^{n}\mathrm{E}\left[(B_{t_{k}}-B_{t_{k-1}})^{2}\right]\\
	&=\sum _{k=1}^{n}(t_{k}-t_{k-1})\\
	&=t
	\end{aligned}
	\]
	and
	\[
	\begin{aligned}
	\mathrm{Var}\left(\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}\right)&=\sum _{k=1}^{n}\mathrm{Var}\left((B_{t_{k}}-B_{t_{k-1}})^{2}\right)\\
	&=\sum _{k=1}^{n}\mathrm{E}\left[(B_{t_{k}}-B_{t_{k-1}})^{4}\right]-\sum _{k=1}^{n}\left(\mathrm{E}\left[\left(B_{t_{k}}-B_{t_{k-1}}\right)^{2}\right]\right)^{2}\\
	&=\sum_{k=1}^{n}3(t_{k}-t_{k-1})^2-\sum _{k=1}^{n}\left(t_{k}-t_{k-1}\right)^{2}\\
	&=2\sum_{k=1}^{n}(t_{k}-t_{k-1})^2\\
	&\le 2\Vert P\Vert\sum_{k=1}^{n}(t_{k}-t_{k-1})\\
	&= 2\Vert P\Vert t.
	\end{aligned}
	\]
	Since
	\[
	\lim\limits_{\Vert P\Vert\to0 }\mathrm{E}\left[\left(\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}-t\right)^2\right]=\lim\limits_{\Vert P\Vert \to0}\mathrm{Var}\left(\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}\right)\le \lim\limits_{\Vert P\Vert\to0 }2\Vert P\Vert t=0,
	\]
	we conclude 
	\[
	[B]_t=\lim_{\Vert P\Vert \rightarrow 0}\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}=t\quad\text{a.s.}
	\]
\end{proof}
\section{$N$-dimensional Brownian Motion}
\begin{definition}[$N$-dimensional Brownian Motion]
	The $N$-dimemsional stochastic process $B=\left(B^{(1)}, B^{(2)}, \cdots, B^{(N)}\right)$ is
	$a$ (standard) $N$-dimensional Brownian motion if the $N$-components $B^{(i)}$ are independent one-dimensional standard Brownian motions.
\end{definition}

\begin{theorem}[Lévy characterisation]
	Let $M=\left(M^{(1)}, M^{(2)}, \cdots, M^{(N)}\right)$ be a $N$-dimensional stochastic process where $M^{(i)}\in\mathscr{M}_0^{\mathrm{loc}}(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$. Then $M$ is an $N$-dimensional Brownian Motion if and only if 
	\[
	\left\langle M^{(i)},M^{(j)}\right\rangle_t=\delta_{ij}t,\qquad\forall t\ge0.
	\]
\end{theorem}

\chapter{Martingale}
\section{Basic Notion}



\begin{definition}[stopping time]
Let $\tau$  be a random variable defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ with values in $T$. Then $\tau$  is called a \emph{stopping time} (with respect to the filtration $(\mathcal{F}_{t})_{t\in T}$), if the following condition holds:
\[
\forall t\in T,\ \{\tau \leq t\}\in {\mathcal {F}}_{t}
\]
or equivalently
\[
X_{t} :=1_{\tau\le t}=\left\{\begin{array}{ll}{1} & {\text { if }  \tau\le t } \\ {0} & {\text { if }\tau >t}\end{array}\right.
\]
is adapted to $(\mathcal{F}_{t})_{t\in T }$.
\end{definition}

\begin{definition}[stopped process]
	Let $X=(X_{t})_{t\in T}$ be a stochastic process defined on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T},\mathrm{P})$ and $\tau$ be a stopping time with respect to the filtration $(\mathcal{F}_{t})_{t\in T}$. The stopping process $X^\tau$ is defined as $(X_{\tau \wedge t})_{t\in T}$, where
	\begin{align*}
	X_{\tau \wedge t}:\Omega&\longrightarrow S\\
	\omega&\longmapsto X_{\tau(\omega) \wedge t}(\omega).
	\end{align*}
\end{definition}
It is useful to observe that, if $\mu$ is another stopping time, then
\[
(X^\tau)^\mu=(X^\mu)^\tau=X^{\mu\wedge\tau}.
\]
\begin{proposition}
	Let $\left(X_{t}\right)_{t \geq 0}$ be an adapted process defined on the filtered probability space $(\Omega,\mathcal{F},\mathbb{F},\mathrm{P})$ with values in a metric space $(E, d)$.
	\begin{enumerate}  
		\item Assume that the sample paths of $X$ are right-continuous, and let $O$ be an open subset of $E$. Then
		\[
		\tau_{O}=\inf \left\{t \geq 0: X_{t} \in O\right\}
		\]
		is a stopping time of the filtration $\mathbb{F}^+$.
		\item Assume that the sample paths of $X$ are continuous, and let $F$ be a closed subset
		of $E$. Then
		\[
		\tau_{F}=\inf \left\{t \geq 0: X_{t} \in F\right\}
		\]
		is a stopping time of the filtration $\mathbb{F}$.
	\end{enumerate}
\end{proposition}


\section{Discrete-time Martingale}
\begin{definition}[discrete-time martingale]
	A discrete-time stochastic process $M=(M_n)_{n\ge 0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ is a \emph{martingale} if it satisfies
	\begin{enumerate}
		\item For $n\ge0$, $\mathrm{E}[|M_n|]<+\infty$;
		\item For $n\ge0$, $\mathrm{E}[M_{n+1}|\mathcal{F}_n]=M_n$.
	\end{enumerate}
\end{definition}
\begin{definition}[discrete-time submartingale]
A discrete-time \emph{submartingale} is a stochastic process $M=(M_n)_{n\ge 0}$ consisting of integrable random variables satisfying 
\begin{enumerate}
	\item For $n\ge0$, $\mathrm{E}[|M_n|]<+\infty$;
	\item For $n\ge0$, $\mathrm{E}[M_{n+1}|\mathcal{F}_n]\ge M_n$.
\end{enumerate}

\end{definition}
\begin{definition}[discrete-time supermartingale]
	A discrete-time \emph{supermartingale} is a stochastic process $M=(M_n)_{n\ge 0}$ consisting of integrable random variables satisfying for $n\ge0$
	\begin{enumerate}
		\item For $n\ge0$, $\mathrm{E}[|M_n|]<+\infty$;
		\item For $n\ge0$, $\mathrm{E}[M_{n+1}|\mathcal{F}_n]\le M_n$.
	\end{enumerate}
\end{definition}

\begin{example}
	Suppose $(M_n)_{n\ge 0}$ is a martingale defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ and that $\phi:\mathbb{R}\to\mathbb{R}$ is convex. If $\phi(M_n)$ is integrable for $n\ge0$, then $\left(\phi(M_n)\right)_{n\ge 0}$ is a submartingale.
\end{example}

\begin{definition}[stopping time in discrete-time case]
	Let $\tau$  be a random variable defined on the probability space $(\Omega,\mathcal{F},\mathrm{P})$ with values in $\mathbb{N}\cup\{+\infty\}$. Then $\tau$ is called a stopping time (with respect to the filtration $(\mathcal{F}_{n})_{n\ge 0}$), if the following condition holds:
	\[
	\forall n\in\mathbb{N},\ \{\tau \le n\}\in {\mathcal {F}}_{n}
	\]
	or equivalently
	\[
	\forall n\in\mathbb{N},\ \{\tau =n\}\in {\mathcal {F}}_{n}.
	\]
\end{definition}
Since $\{\tau=\infty\}^{c}=\bigcup\limits_{n\ge0}\{\tau=n\} \in \mathcal{F}_{\infty}$, we can deduce that $\{\tau=\infty\}\in \mathcal{F}_{\infty}$.
\begin{example}
	Given a discrete-time stochastic process $(X_{n})_{n\ge 0}$ defined on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0 },\mathrm{P})$ and a Borel set $B$,
	\[
	\tau=\inf\{n\ge0:X_n\in B\}
	\]
	is a stopping time of the filtration $(\mathcal{F}_{n})_{n\ge0 }$, called the \emph{first hitting time}. ($\inf\varnothing = \infty $)
\end{example}

\begin{definition}[martingale transform]
The process $\widetilde{M}=(\widetilde{M}_{n})_{n\ge0}$ defined by setting $\widetilde{M}_{0}=M_{0}$ and by setting
\[
\widetilde{M}_{n}=M_{0}+A_{1}\left(M_{1}-M_{0}\right)+A_{2}\left(M_{2}-M_{1}\right)+\cdots+A_{n}\left(M_{n}-M_{n-1}\right)
\]
for $n \geq 1$ is called the martingale transform of $M$ by $A$.
\end{definition}

\begin{theorem}[martingale transform theorem]
If $M=(M_{n})_{n\ge0}$ is a martingale defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ and if $A=(A_{n})_{n\ge0}$ is predictable process with respect to $(\mathcal{F}_{n})_{n\ge0}$, then the martingale transform $\widetilde{M}$ of $M$ by $A$ is itself a martingale with respect to $(\mathcal{F}_{n})_{n\ge0}$.
\end{theorem}

\begin{theorem}[stopping time theorem]
If $M=(M_{n})_{n\ge0}$ is a martingale defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ and $\tau$ is a stopping time with respect to the filtration $(\mathcal{F}_{n})_{n\ge 0}$, then the stopped process $M^\tau=(M_{\tau\wedge{n}})_{n\ge0}$ is also a martingale defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ and $\mathrm{E}[M_{\tau\wedge{n}}]=\mathrm{E}[M_0]$ for $n\ge0$.
\end{theorem}

\begin{theorem}[Doob's optional sampling theorem]
Let $M=(M_{n})_{n\ge0}$ be a martingale defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ and $\tau$ be a stopping time with respect to the filtration $(\mathcal{F}_{n})_{n\ge 0}$. Suppose $\mathrm{P}(\tau<\infty)=1$ and $M^\tau$ is $L^1$-bounded, then $\mathrm{E}[M_\tau]=\mathrm{E}[M_{0}]$.
\end{theorem}
\begin{proof}	
Since $\mathrm{P}(\tau<\infty)=1$, $X_{\tau \wedge n} \stackrel{a.s.}{\longrightarrow}X_{\tau}$ and $\left|X_{\tau}\right| \leq K<\infty$ and hence $\mathrm{E}\left[\left|X_{\tau}\right|\right]<\infty .$ Thus, $\mathrm{E}\left[\left|X_{\tau}-X_{\tau \wedge n}\right| \right]\leq 2 K \mathrm{P}(\tau>n) \rightarrow 0$.
\end{proof}

\section{Continuous-time Martingale}
\begin{definition}[continuous-time martingale]
	A continuous-time stochastic process $M=(M_t)_{t\ge 0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ is a \emph{martingale} if it satisfies
	\begin{enumerate}
		\item For $t\ge0$, $\mathrm{E}[|M_t|]<+\infty$, that is, $M_t$ is $L^1$-bounded;
		\item For $0\le t\le s<+\infty$, $\mathrm{E}[\,M_{s}|\mathcal{F}_t\,]=M_t$.
	\end{enumerate}
\end{definition}
\begin{definition}[continuous martingale]
	A continuous-time martingale $M=(M_t)_{t\ge 0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ is continuous if the paths of $M$ are almost surely continuous. That is, there exists an $\Omega_{0} \subset \Omega$ with $P\left(\Omega_{0}\right)=1$ such that for all $\omega \in \Omega_{0}$ the function
	\begin{align*}
	\gamma_\omega:[0, \infty)&\longrightarrow\mathbb{R}\\
	t&\longmapsto X_{t}(\omega)
	\end{align*}
	is continuous.
\end{definition}

\begin{definition}[$L^p$ martingale]
	A martingale $M=(M_t)_{t\ge0}$ is said to be a $L^p$ \emph{martingale} if for all $t\ge0$, $M_t\in L^p(\Omega,\mathcal{F},\mathrm{P})$ or equivalently
	$$
	\mathrm{E}\left[|M_{t}|^{p}\right]<\infty.
	$$
\end{definition}

\begin{definition}[$L^p$-bounded martingale]
	A martingale $M=(M_t)_{t\ge0}$ is said to be $L^p$-\emph{bounded} if 
	$$
	\sup _{t \geq 0} \mathrm{E}\left[|M_{t}|^{p}\right]<\infty.
	$$
\end{definition}


\begin{definition}[uniform integrability]
	A class $\mathcal {C}$ of random variables is called \emph{uniformly integrable} if given $\varepsilon >0$, there exists $K\in [0,\infty )$ such that 
	\[
	\mathrm{E}\left(|X| 1_{|X| \geq K}\right) \leq \varepsilon \text { for all } X \in \mathcal{C}.
	\]	
\end{definition}

\begin{theorem}[Doob's maximal inequalities in continuous time]
If $M=(M_{t})_{t\ge0}$ is a continuous nonnegative submartingale on $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ and $\lambda>0,$ then for all $p \geq 1$ we have
\[
\lambda^{p} \mathrm{P}\left(\sup_{0 \le t \le T} M_{t}>\lambda\right) \le \mathrm{E}\left[M_{T}^{p}\right]
\]
and, if $M_{T} \in L^{p}(\Omega,\mathcal{F},\mathrm{P})$ for some $p>1,$ then we also have
\[
\left\|\sup_{0 \leq t \leq T} M_{t}\right\|_{p} \leq \frac{p}{p-1}\left\|M_{T}\right\|_{p}.
\]
\end{theorem}



\begin{theorem}[martingale convergence theorems in continuous time]\label{martingale convergence}
Let $M=(M_{t})_{t\ge0}$ be a continuous martingale on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$.
\begin{enumerate}
	\item If $M$ satisfies $\mathrm{E}\left[\left|M_{t}\right|^{p}\right] \leq B<\infty$ for some $p>1$ and all $t \geq 0,$ then there exists a random variable denoted by $M_{\infty}\in L^p(\Omega,\mathcal{F},\mathrm{P})$ with $\mathrm{E}\left[\left|M_{\infty}\right|^p\right] \leq B$ such that
	\[
	M_t\stackrel{\ a.s.\ }{\longrightarrow}M_{\infty}\text { and }M_t\stackrel{\ L^p\ }{\longrightarrow}M_{\infty},\qquad t\longrightarrow\infty
	\]
	or alternatively
	\[
	\mathrm{P}\left(\lim _{t \rightarrow \infty} M_{t}=M_{\infty}\right)=1 \text { and } \lim _{t \rightarrow \infty}\left\|M_{t}-M_{\infty}\right\|_{p}=0.
	\]
	\item If $M$ satisfies $\mathrm{E}\left[\left|M_{t}\right|\right]\le B<\infty$ for all $t \ge 0,$ then there exists a random variable $M_{\infty}\in L^1(\Omega,\mathcal{F},\mathrm{P})$ with $\mathrm{E}\left[\left|M_{\infty}\right|\right] \leq B$ such that
	\[
	M_t\stackrel{\ a.s.\ }{\longrightarrow}M_{\infty},\qquad t\longrightarrow\infty
	\]
	or alternatively
	\[
	\mathrm{P}\left(\lim _{t \rightarrow \infty} M_{t}=M_{\infty}\right)=1.
	\]
\end{enumerate} 
\end{theorem}

According to the theorem \ref{martingale convergence}, $M_{\infty}$ is well defined for any $L^p$-bounded martingale $M$. 

\begin{proposition}[Hilbert spaces $\mathscr{M}^2_0$ and $\mathscr{M}^2_{0,c}$]
Let $\mathscr{M}_0$ denote the collection of all the martingales $M$ defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ with initial value $M_0=0$ a.s.. All the $L^2$-bounded martingales $M\in\mathscr{M}_0$ constitute a Hilbert space, which is denoted by $\mathscr{M}^2_{0}$, with the inner product defined as 
$$
 (M, N)_{\mathscr{M}^2_{0}}:=\left( M_{\infty}, N_{\infty}\right)_{L^{2}}=\mathrm{E}\left[M_{\infty} N_{\infty}\right].
$$
All the $L^2$-bounded continuous martingales $M\in\mathscr{M}_0$ constitute a Hilbert space $\mathscr{M}^2_{0,c}$, which is a closed subspace of $\mathscr{M}^2_{0}$. It follows that $\mathscr{M}^2_{0,c}\subset\mathscr{M}^2_{0}\subset\mathscr{M}_{0}$.
\end{proposition}

\begin{definition}[quadratic variation]	
	Suppose that $X=(X_t)_{t\ge 0}$ is a real-valued stochastic process defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$. The \emph{quadratic variation} of $X$ (if exists) is defined as the stochastic process $\langle X\rangle=(\langle X\rangle_{t})_{t\ge0}$ satisfying that for all $t\ge0$, for all $\varepsilon>0$,
	\[
	\lim_{\left\Vert P_{[0,t]}\right\Vert \rightarrow 0}\mathrm{P}\left(\,\left|\sum _{k=1}^{n}(X_{t_{k}}-X_{t_{k-1}})^{2}-\langle X\rangle_{t}\right|>\varepsilon\right)=0
	\] 
	where $P_{[0,t]}=\{(t_0,\cdots,t_n):0=t_0<t_1<\cdots<t_n=t\}$ ranges over partitions of the interval $[0,t]$ and the norm of the partition $P_{[0,t]}$ is the length of the longest of these subintervals, namely
	\[
	\left\Vert P_{[0,t]}\right\Vert=\max\limits_{1\le k\le n}{|t_k-t_{k-1}|}.
	\]
\end{definition}

\begin{definition}[class D process]
	A process $Z$ is of \emph{class D} if 
	\begin{enumerate}
		\item $Z_0=0$ a.s;
		\item The collection $\left\{Z_{\tau}| \tau \text { is a finite-valued stopping time}\right\}$ is uniformly integrable.
	\end{enumerate}
\end{definition}

\begin{theorem}[Doob–Meyer decomposition theorem]\label{DM}
Let $Z$ be an RCLL submartingale of class D, then there exists a unique, increasing, predictable process $A$ with $A_{0}=0$ such that $M=Z-A$ is a uniformly integrable martingale. $Z=M+A$ is called the Doob–Meyer decomposition of $Z$.
\end{theorem}

\begin{proposition}
	A RCLL nonnegative submartingale $Z$ with $Z_0=0$ is of class D.
\end{proposition}

\begin{definition}[finite variation process]	
	A process $X$ is said to have \emph{finite variation} if it has bounded variation over every finite time interval with probability 1.	
\end{definition}	

The quadratic variation exists for all continuous finite variation processes, and is zero. Let $\mathscr{M}_c^2$ denote the space consisting of all the $L^2$-bounded continuous martingales. The following proposition indicates that the quadratic variation also exists for all martingales in $\mathscr{M}_c^2$.
 
\begin{proposition}[existence of quadratic variation in $\mathscr{M}^2_{c}$]
	If $M\in \mathscr{M}^2_{c}$, then its quadratic variation $\langle M\rangle$ exists and has finite variation. The almost sure limit of $\langle M\rangle_{t}$ as $t \rightarrow \infty$ exists and is denoted by
	\[
	\langle M\rangle_{\infty}=\lim_{t\to\infty}\langle M\rangle_{t}\quad\text{ a.s.}
	\]
	Moreover, $\langle M\rangle_{\infty}$ is integrable, and satisfies
	\[
	\mathrm{E}\left[M_{\infty}^2\right]=\mathrm{E}\left[M_{0}^{2}\right]+\mathrm{E}\left[\langle M\rangle_{\infty}\right].
	\]
\end{proposition}
\begin{proof}
	Let $\widetilde{M}=M-M_0\in\mathscr{M}^2_{0,c}$. Since $\varphi:t\mapsto t^2$ is convex and $\mathrm{E}\left[\widetilde{M}^2_t\right]<+\infty$ for $t\ge0$, we see $\widetilde{M}^2$ is an RCLL submartingale. Since $\widetilde{M}^2$ is nonnegative, proposition \ref{DM} tells that $\widetilde{M}^2$ is of class D. Thus we have the unique Doob-Meyer $\widetilde{M}^2=X+A$, where $X$ is a uniformly integrable martingale and $A$ is an increasing, predictable process. We can show that $A$ is exactly the quadratic variation $\langle M\rangle$.
\end{proof}


\begin{definition}[bracket process]	
	The \emph{bracket process} of two processes $X$ and $Y$ is defined as
	\[
	\langle X, Y\rangle:=\frac{1}{4}\left(\langle X+Y\rangle-\langle X-Y\rangle\right)
	\] 
	if both $\langle X+Y\rangle$ and $\langle X-Y\rangle$ exist.
\end{definition}

\begin{proposition}
	If $M,N\in\mathscr{M}^2_{0,c}$, then $\langle M, N\rangle$ exists and $MN-\langle M, N\rangle$ is a uniformly integrable martingale. Consequently, the almost sure limit of $\langle M, N\rangle_{t}$ as $t \rightarrow \infty$ exists and is denoted by
	\[
	\langle M, N\rangle_{\infty}=\lim_{t\to\infty}\langle M, N\rangle_{t}\quad\text{ a.s.}
	\]
	Moreover, $\langle M, N\rangle_{\infty}$ is integrable, and satisfies
	\[
	\mathrm{E}\left[\langle M, N\rangle_{\infty}\right]=\mathrm{E}\left[M_{\infty} N_{\infty}\right].
	\]
\end{proposition}

\begin{proposition}
For all $\alpha, \beta \in \mathbb{R}$, $M, M^{\prime}, N \in \mathscr{M}^2_{0,c},$

\begin{enumerate}
	\item $\left\langle\alpha M+\beta M^{\prime}, N\right\rangle=\alpha\langle M, N\rangle+\beta\left\langle M^{\prime}, N\right\rangle$
	\item$\langle M, N\rangle=\langle N, M\rangle$
	\item$\langle M, M\rangle=\langle M\rangle \geq 0$ and $\langle M\rangle= 0 \iff M=0$
\end{enumerate}
\end{proposition}

\begin{proposition}
\[
\int_{0}^{t}\left|X_{s}\right|\left|Y_{s}\right| d\langle M, N\rangle_{s} \leq\left(\int_{0}^{t} X_{s}^{2} d\langle M\rangle_{s}\right)^{\frac{1}{2}}\left(\int_{0}^{t} Y_{s}^{2} d\langle N\rangle_{s}\right)^{\frac{1}{2}}.
\]
\end{proposition}

\begin{proposition}
	\[
	\left\langle M^{\tau}, N^{\tau}\right\rangle=\left\langle M^{\tau}, N\right\rangle=\langle M, N\rangle^{\tau}.
	\]
\end{proposition}

\section{Continuous Local Martingale}
\begin{definition}[continuous local martingale]
	An adapted process $M=\left(M_{t}\right)_{t \geq 0}$ with continuous sample paths is called a \emph{continuous local martingale} if there exists a nondecreasing sequence $\left(\tau_{n}\right)_{n \geq 0}$ of stopping times such that $\tau_{n} \uparrow \infty$ and, for every $n,$ the stopped process $M^{\tau_{n}}$ is a martingale.
	
	The sequence of stopping times $\left(\tau_{n}\right)_{n\ge0}$ is called the \emph{localizing sequence}
	for (or is said to reduce) $M$ if $\tau_{n} \uparrow \infty$ and, for every $n,$ the stopped process $M^{\tau_{n}}$ is a martingale.
\end{definition}


\begin{proposition}[linear space $\mathscr{M}^{\mathrm{loc}}_{0,c}$]
	Let $\mathscr{M}_0^{\mathrm{loc}}(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ denote the collection of all the local martingales $M$ defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ with initial value $M_0=0$ a.s.. For simplicity we will just denote $\mathscr{M}^{\mathrm{loc}}_{0,c}$ when the underlying filtered probability space is clear. All the continuous local martingales $M\in\mathscr{M}^{\mathrm{loc}}_{0}$ constitute a vector space, which is denoted by $\mathscr{M}^{\mathrm{loc}}_{0,c}$.
\end{proposition}

\begin{proposition}~\\
	\begin{enumerate}
		\item $\mathscr{M}_{0,c}\subset\mathscr{M}^{\mathrm{loc}}_{0,c}$, and for any $M\in\mathscr{M}_{0,c}$ the sequence $\tau_{n}=n\;(n\ge0)$ reduces $M$. 
		\item A nonnegative continuous local martingale $M$ such that $M_{0} \in L^{1}(\Omega,\mathcal{F}_0,\mathrm{P})$ is a supermartingale.
		\item A continuous local martingale $M$ such that there exists a random variable $Z \in L^{1}(\Omega,\mathcal{F},\mathrm{P})$ with $\left|M_{t}\right| \leq Z$ for every $t \geq 0$ (in particular a bounded continuous local martingale) is a uniformly integrable martingale.
		\item For $M \in \mathscr{M}^{\mathrm{loc}}_{0,c}$ and a stopping time $\tau,$ we have $M^{\tau} \in \mathscr{M}^{\mathrm{loc}}_{0,c}$.
		\item For $M \in \mathscr{M}^{\mathrm{loc}}_{0,c}$, the sequence
		$\tau_{n}=\inf \left\{t \geq 0:\left|M_{t}\right| \geq n\right\}\;(n\ge0)$	reduces $M$.
		\item If $\left(\tau_{n}\right)_{n\ge0}$ reduces $M$ and $\left(\upsilon_{n}\right)_{n\ge0}$ is a sequence of stopping times such that $\upsilon_{n} \uparrow \infty$, then the sequence $\left(\tau_{n} \wedge \upsilon_{n}\right)_{n\ge0}$ also reduces $M$.
	\end{enumerate}
\end{proposition}

\begin{proposition}[existence of quadratic variation]	
	Let $M=\left(M_{t}\right)_{t \geq 0}$ be a continuous local martingale. There exists an	increasing finite variation process $Q=\left(Q\right)_{t \geq 0}$, which is unique up to indistinguishability, such that $(M_{t}^{2}-Q_{t})_{t \geq 0}$ is a continuous local martingale. Furthermore, $Q$ is exactly the quadratic variation of $M$.
\end{proposition}

\begin{proposition}
	If $M,N\in \mathscr{M}^{\mathrm{loc}}_{0,c}$, the bracket process of $M$ and $N$ is well defined as
	\[
	\langle M, N\rangle_{t}:=\frac{1}{4}\left(\langle M+N\rangle_{t}-\langle M-N\rangle_{t}\right).
	\]
	Furthermore, for all $t\ge0$,
	\[
	\langle M, N\rangle_{t}=\mathop{\mathrm{plim}}\limits_{\left\Vert P_{[0,t]}\right\Vert \rightarrow 0} \sum_{k=1}^{n}\left(M_{t_{k}}-M_{t_{k-1}}\right)\left(N_{t_{k}}-N_{t_{k-1}}\right),
	\]
	where $P_{[0,t]}=\{(t_0,\cdots,t_n):0=t_0<t_1<\cdots<t_n=t\}$ ranges over partitions of the interval $[0,t]$ and 
	\[
	\left\Vert P_{[0,t]}\right\Vert=\max\limits_{1\le k\le n}{|t_k-t_{k-1}|}.
	\]
\end{proposition}

\section{Continuous Semimartingales}
\begin{definition}[continuous semimartingale]
A process $X=\left(X_{t}\right)_{t \geq 0}$ is a continuous semimartingale if it can be written in the form
\[
X_{t}=M_{t}+A_{t}
\]
where $M$ is a continuous local martingale and $A$ is a continuous finite variation process.
\end{definition}
The decomposition $X=M+A$ is unique up to indistinguishability.

\begin{definition}[bracket process]
Let $X=M+A$ and $Y=M^{\prime}+A^{\prime}$ be the canonical decompositions of two continuous semimartingales $X$ and $Y$. The bracket $\langle X, Y\rangle$ is the finite variation process defined by
\[
\langle X, Y\rangle_{t}=\left\langle M, M^{\prime}\right\rangle_{t}
\]
In particular, we have $\langle X\rangle_{t}=\langle M\rangle_{t}$.
\end{definition}

\begin{proposition}
	Assume $X$ and $Y$ are two continuous semimartingales. For all $t\ge0$,
	\[
	\sum_{k=1}^{n}\left(M_{t_{k}}-M_{t_{k-1}}\right)\left(N_{t_{k}}-N_{t_{k-1}}\right)\stackrel{p}{\longrightarrow}\langle M, N\rangle_{t},\quad\left\Vert P_{[0,t]}\right\Vert \longrightarrow 0,
	\]
	where $P_{[0,t]}=\{(t_0,\cdots,t_n):0=t_0<t_1<\cdots<t_n=t\}$ ranges over partitions of the interval $[0,t]$ and 
	\[
	\left\Vert P_{[0,t]}\right\Vert=\max\limits_{1\le k\le n}{|t_k-t_{k-1}|}.
	\]
\end{proposition}

\chapter{Stochastic Integration}
If not specified explicitly, the stochastic processes and random variables are always assumed to be defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$.
\section{Stochastic Integrals for $L^2$-Bounded Martingales}


\begin{proposition}[Hilbert space $\mathscr{L}^2(M)$]
	Suppose $M\in \mathscr{M}^2_{0,c}$. Define
	\begin{align*}
	\mathrm{P}_{M}: \mathcal{F} \otimes \mathcal{B}([0, \infty))&\longrightarrow S\\
	A&\longmapsto \mathrm{E}\left[\int_{0}^{\infty} \mathbf{1}_{A}(\omega, s) d\langle M\rangle_{s}\right]=\int_{\Omega}\left[\int_{0}^{\infty} \mathbf{1}_{A}(\omega, s) d\langle M\rangle_{s}(\omega)\right]d\mathrm{P}
	\end{align*}
	Then $\left( \Omega\times[0, \infty) ,  \mathcal{F}\otimes \mathcal{B}([0, \infty)), \mathrm{P}_{M}\right)$ is a measure space. Let 
	$$
	\mathscr{L}^2(M)=\{\Phi\in L^2\left( \Omega\times[0, \infty) ,\mathcal{F}\otimes \mathcal{B}([0, \infty)), \mathrm{P}_{M}\right):\Phi \text{ is progressively measurable}\}.
	$$
	$\mathscr{L}^2(M)$ is a closed subspace of $L^2\left( \Omega\times[0, \infty) ,\mathcal{F}\otimes \mathcal{B}([0, \infty)), \mathrm{P}_{M}\right)$ and also a Hilbert space, with the inner product written as
	\[
	(\Phi,\Psi)_{\mathscr{L}^2(M)}= \mathrm{E}\left[\int_{0}^{\infty} \Phi_s\Psi_s d\langle M\rangle_{s}\right].
	\]
	The associated norm is
	\[
	\|\Phi\|_{\mathscr{L}^2(M)}=\left(\mathrm{E}\left[\int_{0}^{\infty} \Phi_{s}^{2} d\langle M\rangle_{s}\right]\right)^{\tfrac{1}{2}}.
	\]
	Therefore, $\mathscr{L}^2(M)$ consists of all the progressive processes $\Phi$ such that
	\[
	\mathrm{E}\left[\int_{0}^{\infty} \Phi_{s}^{2} d\langle M\rangle_{s}\right]<\infty
	\]
	with the identifications for all processes that only differ on $\mathrm{P}_M$-null sets.
\end{proposition}

\begin{definition}[elementary process]
An elementary process is a progressive process of the form
\[
\Phi_{t}(\omega)=\sum_{i=0}^{p-1} \Phi_{(i)}(\omega) \mathbf{1}_{\left(t_{i}, t_{i+1}\right]}(t)
\]
where $0=t_{0}<t_{1}<t_{2}<\cdots<t_{p}$ and for every $i \in\{0,1, \cdots, p-1\}, \Phi_{(i)}$ is a
bounded $\mathscr{F}_{t_{i}}$ -measurable random variable.
\end{definition}

The set $\mathscr{E}$ of all elementary processes forms a linear subspace of $L^{2}(M) .$ To be precise, we should here say "equivalence classes of elementary processes" "(recall that $\Phi$ and $\Phi^{\prime}$ are identified in $\mathscr{L}^{2}(M)$ if $\left\|\Phi-\Phi^{\prime}\right\|_{\mathscr{L}^{2}(M)}=0$ ).

\begin{proposition}
For every $M \in \mathscr{M}^2_{0,c}, \mathscr{E}$ is dense in $\mathscr{L}^{2}(M)$.
\end{proposition}

\begin{theorem}
Let $M \in \mathscr{M}^2_{0,c} .$ For every $\Phi \in \mathscr{E}$ of the form
\[
\Phi_{t}(\omega)=\sum_{i=0}^{p-1} \Phi_{(i)}(\omega) \mathbf{1}_{\left(t_{i}, t_{i+1}\right]}(t)
\]
the formula
\[
(\Phi \cdot M)_{t}=\int_{0}^{t}\Phi_sdM_s:=\sum_{i=0}^{p-1} \Phi_{(i)}\left(M_{t_{i+1} \wedge t}-M_{t_{i} \wedge t}\right)
\]
defines a process $\Phi \cdot M \in \mathscr{M}^2_{0,c}.$ The mapping $I_M^*:\mathscr{E}\to\mathscr{M}^2_{0,c},\;\Phi \mapsto \Phi \cdot M$ can extend to a linear isometry
\begin{align*}
I_M:\mathscr{L}^{2}(M)&\longrightarrow\mathscr{M}^2_{0,c}\\
\Phi &\longmapsto \Phi \cdot M,
\end{align*}
which means
\[
\|\Phi \cdot M\|_{\mathscr{M}^2_c}=\left(\mathrm{E}\left[\left(\int_{0}^{\infty}\Phi_sdM_s\right)^2\right]\right)^{\tfrac{1}{2}}=\|\Phi\|_{\mathscr{L}^2(M)}=\left(\mathrm{E}\left[\int_{0}^{\infty} \Phi_{s}^{2} d\langle M\rangle_{s}\right]\right)^{\tfrac{1}{2}}.
\]
Furthermore, $\Phi \cdot M$ is the unique martingale in $\mathscr{M}^2_{0,c}$ that satisfies the property
\begin{align*}
\langle \Phi \cdot M, N\rangle&= \Phi \cdot\langle M, N\rangle, \quad \forall N \in\mathscr{M}^2_{0,c},\\
\left\langle \int_{0}^{\cdot}\Phi_sdM_s, N\right\rangle_t&= \int_{0}^{t}\Phi_sd\left\langle M, N\right\rangle_s, \quad \forall N \in\mathscr{M}^2_{0,c},\;t\in[0,\infty).
\end{align*}
We call $\Phi \cdot M$ the stochastic integral of $\Phi$ with respect to $M$.
\end{theorem}

\begin{proposition}
	Assume that $M,N\in\mathscr{M}^2_{0,c}$, $\Phi\in\mathscr{L}^2(M)$, $\Psi\in\mathscr{L}^2(N)$. Then
	\[
	\left\langle\int_{0}^\cdot \Phi_{s} \mathrm{d} M_{s}, \int_{0}^\cdot \Psi_{s} \mathrm{d} N_{s}\right\rangle_{t}=\int_{0}^{t} \Phi_{s}\Psi_{s} \mathrm{d}\langle M, N\rangle_{s},\quad\forall t\in[0,\infty].
	\]
\end{proposition}

\begin{proposition}
If $\tau$ is a stopping time with respect to $(\mathcal{F}_t)_{t\ge0}$, we have
\begin{align*}
\left(\mathbf{1}_{[0, \tau]} \Phi\right) \cdot M&=(\Phi \cdot M)^{\tau}=\Phi \cdot M^{\tau},\\
\int_{0}^{t}\mathbf{1}_{[0, \tau]}(s)\Phi_sdM_s&=\int_{0}^{\tau\wedge t}\Phi_sdM_s=\int_{0}^{ t}\Phi_sdM_s^{\tau},\quad\forall t\in[0,\infty].
\end{align*}
\end{proposition}

\section{Stochastic Integrals for Continuous Local Martingales}

We will now use extend the definition of $\Phi \cdot M$ to an arbitrary continuous local martingale. If $M\in\mathscr{M}^{\mathrm{loc}}_{0,c}$, we write $\mathscr{L}_{\mathrm{loc}}^{2}(M)$ for the set of all progressive processes $\Phi$ such that for all $t\ge0$,
\[
\int_{0}^{t} \Phi_{s}^{2} \mathrm{d}\langle M\rangle_{s}<\infty.
\]
For future reference, we note that $\mathscr{L}_{\mathrm{loc}}^{2}(M)$ can again be viewed as an ``ordinary" $L^{2}$-space and thus has a Hilbert space structure. Clearly we see $\mathscr{L}^{2}(M)\subset\mathscr{L}_{\mathrm{loc}}^{2}(M)$ for $M\in\mathscr{M}^{2}_{0,c}$.


\begin{theorem}[stochastic integrals for continuous local martingales]
	Let $M\in\mathscr{M}^{\mathrm{loc}}_{0,c}$. For every $\Phi \in \mathscr{L}_{\mathrm{loc}}^{2}(M)$ there exists a unique continuous local martingale in $\mathscr{M}^{\mathrm{loc}}_{0,c}$ , which is denoted by $\Phi \cdot M$ or $\int_{0}^{\cdot}\Phi_sdM_s$, such that for every $N\in\mathscr{M}^{\mathrm{loc}}_{0,c}$,
	\[
	\langle \Phi \cdot M, N\rangle= \Phi \cdot\langle M, N\rangle.
	\]
	If $M'\in\mathscr{M}^{\mathrm{loc}}_{c}$, then $M'-M_0\in \mathscr{L}_{\mathrm{loc}}^{2}(M)$ and we can define
	\[
	\int_{0}^{\cdot}\Phi_sdM'_s:=\Phi\cdot(M'-M_0).
	\]
	If $\Phi \in \mathscr{L}_{\text {loc }}^{2}(M)$ and $\Psi$ is a progressive process, we have $\Psi \in \mathscr{L}_{\text {loc }}^{2}(\Phi \cdot M)$ if and only if $\Phi \Psi \in \mathscr{L}_{\mathrm{loc}}^{2}(M),$ and then
	\[
	\Phi \cdot(\Psi \cdot M)=(\Phi \Psi) \cdot M.
	\]
	Finally, if $M\in\mathscr{M}^{2}_{0,c}$, and $\Phi \in\mathscr{L}^{2}(M),$ the definition of $\Phi \cdot M$ is consistent with that of Theorem 5.4.
\end{theorem}

\begin{proposition}
	If $\tau$ is a stopping time with respect to $(\mathcal{F}_t)_{t\ge0}$, we have
	\[
	\left(\mathbf{1}_{[0, \tau]} \Phi\right) \cdot M=(\Phi \cdot M)^{\tau}=\Phi \cdot M^{\tau}.
	\]
\end{proposition}

\section{Stochastic Integrals for Continuous Semimartingales}
We finally extend the definition of stochastic integrals to continuous semimartingales. 
\begin{definition}[locally bounded]
We say that a progressive process $\Phi$ is locally bounded if
\[
\forall t \geq 0, \quad \sup _{s \leq t}\left|\Phi_{s}\right|<\infty \quad \text { a.s. }
\]
or equivalently there exist a sequence of stopping times $(\tau_{n})_{n \geq 0}$ and a sequence of constants $(C_n)_{n \geq 0}$ such that
\[
\forall n \geq 0, \quad \forall t \geq 0, \quad \left|\Phi_{t}^{\tau_{n}}\right| \leq C_{n}\quad \text { a.s. }
\]
\end{definition}

In particular, any adapted process with continuous sample paths is a locally bounded progressive process. If $\Phi$ is (progressive and) locally bounded, then for every finite variation process $V,$ we have
\[
\forall t \geq 0, \quad \int_{0}^{t}\left|\Phi_{s}\right|\left|\mathrm{d} V_{s}\right|<\infty, \quad \text { a.s. }
\]
and similarly $\Phi \in \mathscr{L}_{1 \mathrm{oc}}^{2}(M)$ for every continuous local martingale $M$.

\begin{definition}[stochastic integrals for continuous semimartingales]
	Let $X$ be a continuous semimartingale and let $X=M+V$ be its canonical decomposition. If $\Phi$ is a locally bounded progressive process, the stochastic integral $\Phi \cdot X$ is the continuous semimartingale with canonical decomposition
	\[
	\Phi \cdot X=\Phi \cdot M+\Phi \cdot V
	\]
	and we write
	\[
	(\Phi \cdot X)_{t}=\int_{0}^{t} \Phi_{s} \mathrm{d} X_{s}
	\]
\end{definition}


\begin{proposition}
	\begin{enumerate}
		\item The mapping $(\Phi, X) \mapsto \Phi \cdot X$ is bilinear.
		\item $\Phi \cdot(\Psi \cdot X)=(\Phi \Psi) \cdot X,$ if $\Phi$ and $\Psi$ are progressive and locally bounded.
		\item For every stopping time $\tau$, $(\Phi \cdot X)^{\tau}=\Phi \mathbf{1}_{[0, \tau]} \cdot X=\Phi \cdot X^{\tau}$.
		\item If $X$ is a continuous local martingale, resp. if $X$ is a finite variation process, then the same holds for $\Phi \cdot X$.
		\item If $H$ is of the form 
		$$
		\Phi_{s}(\omega)=\sum_{i=0}^{p-1} \Phi_{(i)}(\omega) \mathbf{1}_{\left(t_{i}, t_{i}+1\right)}(s),
		$$
		where $0=t_{0}<t_{1}<$
		$\cdots<t_{p},$ and, for every $i \in\{0,1, \cdots, p-1\}, H_{(i)}$ is $\mathcal{F}_{t_{i}}$-measurable, then
		\[
		\int_{0}^{t} \Phi_{s} \mathrm{d} X_{s}=\sum_{i=0}^{p-1} \Phi_{(i)}\left(X_{t_{i+1} \wedge t}-X_{t_{i} \wedge t}\right).
		\]
	\end{enumerate}
\end{proposition}


\begin{proposition}
	Assume that $\Phi$ is a process with continuous sample paths and that $X$ is continuous semimartingales. Then, for every $t>0$,
	\[
	\sum_{k=1}^{n} \Phi_{t_{k-1}}\left(X_{t_{k}}-X_{t_{k-1}}\right)\stackrel{p}{\longrightarrow}\int_{0}^{t} \Phi_{s} \mathrm{d} X_{s},\quad\left\Vert P_{[0,t]}\right\Vert \longrightarrow 0,
	\]
	where $P_{[0,t]}=\{(t_0,\cdots,t_n):0=t_0<t_1<\cdots<t_n=t\}$ ranges over partitions of the interval $[0,t]$ and 
	\[
	\left\Vert P_{[0,t]}\right\Vert=\max\limits_{1\le k\le n}{|t_k-t_{k-1}|}.
	\]
\end{proposition}

\section{Itô's Formula}
\begin{theorem}[Itô's formula]
	Let $X^{1}, \cdots, X^{p}$ be $p$ continuous semimartingales, and let $F\in C^2(\mathbb{R}^{p})$ be a twice continuously differentiable real function. Then, for every $t \geq 0$
	\[
	\begin{aligned}
	F\left(X_{t}^{1}, \cdots, X_{t}^{p}\right)=F\left(X_{0}^{1}, \cdots, X_{0}^{p}\right) &+\sum_{i=1}^{p} \int_{0}^{t} \frac{\partial F}{\partial x^{i}}\left(X_{s}^{1}, \cdots, X_{s}^{p}\right) d X_{s}^{i} \\
	&+\frac{1}{2} \sum_{i, j=1}^{p} \int_{0}^{t} \frac{\partial^{2} F}{\partial x^{i} \partial x^{j}}\left(X_{s}^{1}, \cdots, X_{s}^{p}\right) d\left\langle X^{i}, X^{j}\right\rangle_{s}.
	\end{aligned}
	\]
\end{theorem}
Thus we see that if we apply a twice continuously differentiable function $F$ to a $p$-tuple of continuous semimartingales $(X^{1}, \cdots, X^{p})$, the resulting process $F(X^{1}, \cdots, X^{p})$ is still a continuous semimartingale.

\begin{corollary}[formula of integration by parts]
	Take $p=2$ and $F(x, y)=x y$ in the theorem. If $X$ and $Y$ are two continuous semimartingales, we have
	\[
	X_{t} Y_{t}=X_{0} Y_{0}+\int_{0}^{t} X_{s} \mathrm{d} Y_{s}+\int_{0}^{t} Y_{s} \mathrm{d} X_{s}+\langle X, Y\rangle_{t}.
	\]
	In particular, if $Y=X$
	\[
	X_{t}^{2}=X_{0}^{2}+2 \int_{0}^{t} X_{s} \mathrm{d} X_{s}+\langle X\rangle_{t}.
	\]
\end{corollary}

\begin{corollary}
	\begin{enumerate}
		\item Let $F\in C^2(\mathbb{R})$.
		$$F\left(B_{t}\right)=F\left(B_{0}\right)+\int_{0}^{t} F^{\prime}\left(B_{s}\right) d B_{s}+\frac{1}{2} \int_{0}^{t} F^{\prime \prime}\left(B_{s}\right) d s.$$
		\item Let $F(t,x)\in C^2(\mathbb{R}^2)$.
		\[
		F\left(t, B_{t}\right)=F\left(0, B_{0}\right)+\int_{0}^{t} \frac{\partial F}{\partial x}\left(s, B_{s}\right) d B_{s}+\int_{0}^{t}\left(\frac{\partial F}{\partial t}+\frac{1}{2} \frac{\partial^{2} F}{\partial x^{2}}\right)\left(s, B_{s}\right) d s
		\]
	\end{enumerate}
	
	
\end{corollary}

A random process with values in the complex plane $\mathbb{C}$ is called a \emph{complex continuous local martingale} if both its real part and its imaginary part are continuous local martingales.

\begin{proposition}
	Let M be a continuous local martingale and, for every $\lambda \in \mathbb{C},$ let
	\[
	\mathcal{E}(\lambda M)_{t}=\exp \left(\lambda M_{t}-\frac{\lambda^{2}}{2}\langle M\rangle_{t}\right)
	\]
	The process $\mathcal{E}(\lambda M)$ is a complex continuous local martingale, which can be written
	in the form
	\[
	\mathcal{E}(\lambda M)_{t}=\mathrm{e}^{\lambda M_{0}}+\lambda \int_{0}^{t} \mathcal{E}(\lambda M)_{s} \mathrm{d} M_{s}
	\]
\end{proposition}

\newpage

\chapter*{Appendix}



\section*{1.Properties of Common Distributions}
\makegapedcells
\setcellgapes{3pt}
\newcommand{\minitab}[2][l]{\begin{tabular}{#1}#2\end{tabular}} 
%\newsavebox{\mybox}
%\newcolumntype{X}[1]{>{\begin{lrbox}{\mybox}}c<{\end{lrbox}\makecell[#1]{\fbox{\usebox\mybox}}}}
%\renewcommand\arraystretch{1.5}
\begin{table}[H]
	\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\rowcolor[HTML]{C0C0C0} 
			\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Distribution}& \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}pmf $\mathrm{P}(X=k)$}& \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Support}   & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Variance} \\ \hline
			Bernoulli $B(1,p)$&$p^{k}(1-p)^{1-k}$&$\{0,1\}$&$p$&$p(1-p)$\\  \hline			
			Binomial $B(n,p)$ &${n \choose k}\,p^{k}(1-p)^{n-k}$&$\{0,\cdots, n\}$&$np$&$np(1-p)$ \\   \hline
			Negative Binomial $NB(r,p)$&${k+r-1 \choose k} (1-p)^{r}p^{k}$&$\mathbb{N}$&${\dfrac{pr}{1-p}}$&${\dfrac {pr}{(1-p)^{2}}}$\\ \hline
			Poisson $Pois(\lambda )$ &$\dfrac{\lambda ^{k}e^{-\lambda}}{k!}$
			&$\mathbb{N}$&$\lambda$&$\lambda$\\ \hline
			Geometric $Geo(p)$&$(1-p)^{k-1}p$&$\mathbb{N}_+$&$\dfrac{1}{p}$&$\dfrac{1-p}{p^2}$\\  \hline
			Hypergeometric $H(N,K,n)$&$\dfrac{{K \choose k}{{N-K}\choose {n-k}}}{{N \choose n}}$&$\{0,\cdots
			,\min{(n,K)}\}$&$n\dfrac{K}{N}$&$n\dfrac{K}{N}\dfrac{N-K}{N}\dfrac{N-n}{N-1}$\\ \hline
			Uniform (discrete) $DU(a,b)$&$\dfrac{1}{n}$&$\{a,a+1,\dots,b\}$&$\dfrac{a+b}{2}$&$\dfrac{(b-a+1)^{2}-1}{12}$\\ \hline
		\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Distribution}& \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}pdf}  & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Variance} \\ \hline
		Degenerate $\delta_a$&$I_{\{a\}}(x)$&$a$&0 \\ \hline
		Uniform (continuous) $U(a,b)$&$\dfrac{1}{b-a}I_{[a,b]}(x)$&$\dfrac{a+b}{2}$&$\dfrac{(b-a)^{2}}{12}$\\ \hline
		Exponential $Exp(\lambda)=\Gamma(1,\lambda )$&$\lambda e^{-\lambda x}I_{[0,+\infty)}(x)$&$\lambda^{-1}$ &$\lambda^{-2}$\\ \hline
		Normal $N(\mu ,\sigma ^{2})$&${\dfrac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}$&$\mu$&$\sigma^2$\\ \hline
		Log-normal $LogN(\mu ,\sigma ^{2})$&$\dfrac{1}{x\sqrt{2 \pi\sigma^{2}}} \exp \left(-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}\right)$&$e^{\gamma+\frac{\sigma^{2}}{2}}$&$e^{2\left(\gamma+\sigma^{2}\right)}-e^{2 \gamma+\sigma^{2}}$\\ \hline
		Gamma $\Gamma (\alpha,\beta )$&$\dfrac{\beta ^{\alpha }}{\Gamma (\alpha )}x^{\alpha -1}e^{-\beta x}I_{(0,+\infty)}(x)$&$\dfrac{\alpha}{\beta}$&$\dfrac{\alpha}{\beta^2}$ \\ \hline
		Beta $B(\alpha,\beta )$&$\dfrac{x^{\alpha-1}(1-x)^{\beta-1}}{\mathrm{B}(\alpha, \beta)}I_{(0,1)}(x)$&$\dfrac{\alpha}{\alpha+\beta}$&$\dfrac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}$ \\ \hline
		Chi-squared $\chi^{2}_k=\Gamma (\frac{k}{2},\frac{1}{2})$ &$\dfrac{1}{2^{k/2}\Gamma (k/2)}\;x^{k/2-1}e^{-x/2}I_{(0,+\infty)}(x)$&$k$&$2k$ \\ \hline
		Student's t $t_{\nu}$&$\dfrac { \Gamma \left( \frac { \nu + 1 } { 2 } \right) } { \sqrt { \nu \pi } \Gamma \left( \frac { \nu } { 2 } \right) } \left( 1 + \dfrac { x ^ { 2 } } { \nu } \right) ^ { - \frac { \nu + 1 } { 2 } }$&0&$\dfrac { \nu } { \nu - 2 }$ for $\nu > 2$
		\\ \hline
	\end{tabular}
\end{table}

\section*{2.Generating Function \& Characteristic Function}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Distribution} &  \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Moment-generating function } & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Characteristic function} \\ \hline
		Degenerate $\delta_a$ &${ e ^ { t a } }$ &$ e ^ { i t a }$  \\ \hline
		Bernoulli $B(1,p)$  &${ 1 - p + p e ^ { t } }$ & ${ 1 - p + p e ^ { i t } }$ \\  \hline
		\multirow{2}{*}[-3pt]{Geometric $Geo(p)$} &${ \dfrac { p e ^ { t } } { 1 - ( 1 - p ) e ^ { t } } },$& \multirow{2}{*}[-3pt]{$\dfrac{ pe^{it} }{1 - ( 1 - p ) e^ { it } } $} \\ 
		 &$ t < - \operatorname { l n } ( 1 - p )$& \\  \hline
		Binomial $B(n,p)$ &${ ( 1 - p + p e ^ { t } ) ^ { n } }$ & $ ( 1 - p + p e ^ { i t } ) ^ { n } $ \\   \hline
		Negative Binomial $NB(r,p)$  &$ \dfrac { ( 1 - p ) ^ { r } } { ( 1 - p e ^ { t } ) ^ { r } } $ & $ \dfrac { ( 1 - p ) ^ { r } } { ( 1 - p e ^ { i t } ) ^ { r } } $\\ \hline
		Poisson $Pois(\lambda )$ &$ e ^ { \lambda \left( e ^ { t } - 1 \right) } $ & $ e ^ { \lambda \left( e ^ { i t } - 1 \right) } $ \\ \hline
		Uniform (continuous) $U(a,b)$&$\begin{cases}{\dfrac{\mathrm {e} ^{tb}-\mathrm{e}^{ta}}{t(b-a)}}&{\text{for }}t\neq 0\\1&{\text{for }}t=0\end{cases}$ & $\begin{cases}{\dfrac{\mathrm {e} ^{itb}-\mathrm{e}^{ita}}{it(b-a)}}&{\text{for }}t\neq 0\\1&{\text{for }}t=0\end{cases}$\\ \hline
		Uniform (discrete) $DU(a,b)$&$ \dfrac { e ^ { a t } - e ^ { ( b + 1 ) t } } { ( b - a + 1 ) \left( 1 - e ^ { t } \right) } $ & $ \dfrac { e ^ { i t \mu } } { ( b - a + 1 ) \left( 1 - e ^ { i t } \right) } $\\ \hline
		Laplace $L(\mu ,b)$ &$\dfrac { e ^ { t \mu } } { 1 - b ^ { 2 } t ^ { 2 } } ,\ | t | < 1 / b $ & $ \dfrac { e ^ { i t \mu } } { 1 + b ^ { 2 } t ^ { 2 } } $ \\ \hline
		Normal $N(\mu ,\sigma ^{2})$&$ e ^ { t \mu + \frac { 1 } { 2 } \sigma ^ { 2 } t ^ { 2 } } $ & $ e ^ { i t \mu - \frac { 1 } { 2 } \sigma ^ { 2 } t ^ { 2 } } $\\ \hline
		Chi-squared $\chi^{2}_k$&$( 1 - 2 t ) ^ { - \frac { k } { 2 } } $&$ ( 1 - 2 i t ) ^ { - \frac { k } { 2 } } $ \\ \hline
		Noncentral chi-squared $\chi^{2}_k(\lambda)$&$e^{\lambda t/( 1 - 2 t ) }( 1 - 2 t )^{- \frac{ k } { 2 } } $ & $ e ^ { i \lambda t / ( 1 - 2 i t ) } ( 1 - 2 i t ) ^{ - \frac { k } { 2 } } $ \\ \hline
		Gamma $\Gamma (\alpha,\beta )$&$\left(1-{\dfrac{t}{\beta }}\right)^{-\alpha },\ t<\beta $ & $\left(1-{\dfrac{it}{\beta }}\right)^{-\alpha }$\\ \hline
		Beta $B(\alpha,\beta )$&$1+\sum\limits_{k=1}^{\infty}\left(\prod\limits_{r=0}^{k-1} \dfrac{\alpha+r}{\alpha+\beta+r}\right) \dfrac{t^{k}}{k !}$ & $_{1} F_{1}(\alpha ; \alpha+\beta ; i t)$\\ \hline
		Exponential $Exp(\lambda )$&$\dfrac{\lambda}{\lambda-t}, t < \lambda$ & $\dfrac{\lambda}{\lambda-it}$\\ \hline
		Multivariate normal $N(\bm{\mu } ,\mathbf{\Sigma })$&$e ^ { \mathbf{t} ^ { T } \left( \bm{\mu} + \frac { 1 } { 2 } \mathbf{\Sigma } \mathbf{t} \right) }$&$e ^ { \mathbf{t} ^ { T } \left( i\bm{\mu} -\frac { 1 } { 2 } \mathbf{\Sigma } \mathbf{t} \right) }$ \\ \hline
		Cauchy $Cauchy(\mu ,\theta )$&Does not exist&$e ^ { i t \mu - \theta | t | }$\\ \hline
		Multivariate Cauchy &\multirow{2}{*}[-1.5pt]{Does not exist} &\multirow{2}{*}[-1.5pt]{$e ^ { i \mathbf { t } ^ { \mathrm { T } } \boldsymbol { \mu } - \sqrt { \mathbf { t } ^ { \mathrm { T } } \boldsymbol { \Sigma } \mathbf { t } } }$}\\
		$MultiCauchy(\bm{\mu } ,\mathbf{\Sigma })$&&\\ \hline
	\end{tabular}
\end{table}

\newpage

\begin{thebibliography}{99}  
	\bibitem{ref1}Gall J F L. Brownian Motion, Martingales, and Stochastic Calculus[J]. 2018.
	\bibitem{ref2}Morimoto H. Stochastic control and mathematical modeling[M]. Cambridge University Press, 2010.
	\bibitem{ref3}Athreya K B, Lahiri S N. Measure theory and probability theory[M]. Springer Science \& Business Media, 2006.  
	\bibitem{ref4}Pivato M. Stoshastic processes and stochastic integration[J]. 1999.
\end{thebibliography}

\end{document}
