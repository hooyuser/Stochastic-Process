\documentclass{article}
% Comment the following line to NOT allow the usage of umlauts
\usepackage[utf8]{inputenc}
% Uncomment the following line to allow the usage of graphics (.png, .jpg)
\usepackage{geometry}
\geometry{left=3cm,right=3cm,top=3cm,bottom=3cm}

\usepackage[usenames,dvipsnames]{color}
\usepackage[colorlinks,linkcolor=NavyBlue,anchorcolor=red,citecolor=green]{hyperref}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{float}
\usepackage{bm}
\usepackage{array,makecell}
\usepackage[table,xcdraw]{xcolor}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage[thmmarks,amsmath]{ntheorem}
\theorembodyfont{\upshape}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\normalfont}
\theoremsymbol{\\ \rightline{$\square$}}
\newtheorem{proof}{Proof.}

% Start the document
\begin{document}
	\begin{center}
	\textsc{\Huge Stochastic Process}	
\end{center}
\vspace{1em} 

% Create a new 1st level heading
\section{Preliminaries}
\begin{definition}[stochastic process]
For a given probability space $(\Omega ,{\mathcal{F}},\mathrm{P})$ and a measurable space $(S,\mathcal{E})$, a \emph{stochastic process} is a collection of $S$-valued random variables on $(\Omega ,{\mathcal{F}},\mathrm{P})$ indexed by some set $T$, which can be written as $X=\{X(t):t\in T\}$ or $X=(X_t)_{t\in T}$ or $X:\Omega \times T\rightarrow S$. This mathematical space $S$ is called its state space. 
\end{definition}

\noindent For convenience, we always assume $T$ is a totally ordered set and denote the collection of all finite subsets of $T$ by $\mathcal{I}_T$, namely
\[
\mathcal{I}_T=\{\{t_1,t_2,\cdots,t_n\}:t_1,\cdots,t_n\in T,\;n\ge 1\}.
\]

\begin{definition}[strictly stationary process]
	Let $(X_t)_{t\in T}$ be a stochastic process and let \newline $F_{{X}}(x_{{t_{1}+\tau }},\ldots ,x_{{t_{k}+\tau }})$ represent the  distribution function of the joint distribution of $(X_t)_{t\in T}$ at times $t_{1}+\tau ,\ldots ,t_{k}+\tau$ . Then, $(X_t)_{t\in T}$ is said to be strictly stationary if, for all $k$, for all $\tau$, and for all $ t_{1},\ldots ,t_{k}$,
	\[
	F_{{X}}(x_{{t_{1}+\tau }},\ldots ,x_{{t_{k}+\tau }})=F_{{X}}(x_{{t_{1}}},\ldots ,x_{{t_{k}}}).
	\]
\end{definition}

\begin{definition}[independent increments]
	A stochastic process $(X_t)_{t\in T}$ has \emph{independent increments} if for every $n\in \mathbb{N}_+$ and any $t_1\le t_2 \le\cdots\le t_n$, the increments $X_{t_2}-X_{t_1},X_{t_3}-X_{t_2},\cdots,X_{t_n}-X_{t_{n-1}}$ are independent.
\end{definition}

\begin{definition}[stationary increments]
	A stochastic process $(X_t)_{t\in T}$ has \emph{stationary increments} if for all $s<t$, the probability distribution of the increments $X_{t}-X_{s}$ depends only on $t-s$.
\end{definition}

\section{Poisson Process}
\begin{definition}[Poisson process (I)]
	A stochastic process $(N_t)_{t\ge0}$ defined on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ is said to be a \emph{Poisson process} with rate $\lambda>0$ if
	\begin{enumerate}[(i)]
		\item $N_0=0$;
		\item \hypertarget{Definition 2.1(ii)}{} $(N_t)_{t\ge0}$ has independent increments: for any $n\in \mathbb{N}_+$ and any $0\le t_1\le t_2 \le\cdots\le t_n$, the increment $N_{t_2}-N_{t_1},N_{t_3}-N_{t_2},\cdots,N_{t_n}-N_{t_{n-1}}$ are independent;
		\item \hypertarget{Definition 2.1(iii)}{}for any $0\le s < t$, $N_t-N_s\sim Pois(\lambda(t-s))$, that is 
		\[
		\mathrm{P}(N_t-N_s=k)=e^{-\lambda(t-s)}\dfrac{\lambda(t-s)^k}{k!}\quad(k=0,1,2,\cdots).
		\]
	\end{enumerate}	
\end{definition}

\begin{definition}[counting process]
	A \emph{counting process} is a stochastic process $(N_t)_{t\ge 0}$ with values that are non-negative, integer, and non-decreasing:
	\begin{enumerate}[(i)]
		\item $N_t\ge0$;
		\item $N_t$ is an integer;
		\item If $0\le s\le t$, then $N_s \le N_t$.
	\end{enumerate}	
		
\end{definition}
For any $0\le s<t$, the counting process $N_t-N_s$ represents the number of events that occurred on $(s,t]$.  


\begin{definition}[Poisson process (II)]
	A counting process $(N_t)_{t\ge0}$ defined on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ is said to be a \emph{Poisson process} with rate $\lambda>0$ if
	\begin{enumerate}[(i)]
		\item $N_0=0$;
		\item $(N_t)_{t\ge0}$ has independent increments;
		\item \hypertarget{Definition 2.3(iii)}{}For all $t\ge0$, $\mathrm{P}(N_{t+h}-N_t=1)=\lambda h+o(h)$ when $h\to0$;
		\item \hypertarget{Definition 2.3(iv)}{}For all $t\ge0$, $\mathrm{P}(N_{t+h}-N_t\ge2)=o(h)$ when $h\to0$;
	\end{enumerate}	
\end{definition}

\begin{definition}[Poisson process (III)]
	A stochastic process $(N_t)_{t\ge0}$ defined on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ is said to be a \emph{Poisson process} with rate $\lambda>0$ if
	\[
	N_t=\sum_{n=1}^{\infty}nI_{[T_n,T_{n+1})}(t),
	\]
	where $T_n=X_1+X_2+\cdots+X_n$ and $X_i\text{ i.i.d }\sim Exp(\lambda)$ (Here the pdf of $Exp(\lambda)$ is taken as $\lambda e^{-\lambda x}I_{(0,+\infty)}(x)$).
\end{definition}

\begin{proposition}
	Definition 2.1, 2.3, 2.4 are equivalent definitions of Poisson process.
\end{proposition}
\begin{proof}~\\
	\vspace{-1em}
	\begin{itemize}
	\item \underline{Definition 2.1 $\implies$ Definition 2.3} 
	
	Here we are only to show the implication of \hyperlink{Definition 2.3(iii)}{Definition 2.3(iii)} and \hyperlink{Definition 2.3(iv)}{Definition 2.3(iv)}. Since $N_{t+h}-N_t\sim Pois(\lambda h)$, when $h\to0$ we have 
	\begin{align*}
		\mathrm{P}(N_{t+h}-N_t=1)&=e^{-\lambda h}\lambda h=(1-\lambda h+o(h))\lambda h=\lambda h+o(h),\\
		\mathrm{P}(N_{t+h}-N_t\ge2)&=1-\mathrm{P}(N_{t+h}-N_t=0)-\mathrm{P}(N_{t+h}-N_t=1)\\
		&=1-e^{-\lambda h}-e^{-\lambda h}\lambda h\\
		&=1-(1-\lambda h+o(h))-(\lambda h+o(h))\\
		&=o(h).
	\end{align*}
	\item\underline{Definition 2.3 $\implies$ Definition 2.1} 
	
	Only \hyperlink{Definition 2.1(iii)}{Definition 2.1(iii)} needs to be derived. Given the Laplace transform of the nonnegative random variables $N_t$ and $N_{t+h}$
	\[
	L_{N_t}(u)=\mathrm{E}[e^{-uN_t}],\quad
	L_{N_{t+h}}(u)=\mathrm{E}[e^{-uN_{t+h}}],\quad u\ge0,
	\]
	according to Definition 2.3(ii) we can obtain
	\begin{align*}
	L_{N_{t+h}}(u)&=\mathrm{E}[e^{-uN_{t+h}}]\\
	&=\mathrm{E}[e^{-uN_{t}}e^{-u(N_{t+h}-N_t)}]\\
	&=\mathrm{E}[e^{-uN_{t}}]\mathrm{E}[e^{-u(N_{t+h}-N_t)}]\\
	&=L_{N_t}(u)\mathrm{E}[e^{-u(N_{t+h}-N_t)}].
	\end{align*}
	Note that 
	\begin{align*}
	&\mathrm{E}[e^{-u(N_{t+h}-N_t)}]\\
	=\ &e^{0}\mathrm{P}(N_{t+h}-N_t=0)+e^{-u}\mathrm{P}(N_{t+h}-N_t=1)+\sum_{j=2}^{\infty}e^{-un}\mathrm{P}(N_{t+h}-N_t=j)\\
	=\ &1-\lambda h+o(h)+e^{-u}(\lambda h+o(h))+o(h)\\
	=\ &1-\lambda h+e^{-u}\lambda h+o(h)\quad(h\to 0).
	\end{align*}
	Denote $g(t+h)=L_{N_{t+h}}(u)$ and $g(t)=L_{N_{t}}(u)$ for some fixed $u$ and then we get 
	\[
	\frac{g(t+h)-g(t)}{h}=\frac{g(t)(1-\lambda h+e^{-u}\lambda h+o(h))-g(t)}{h}=g(t)\lambda (e^{-u}-1)+\frac{o(h)}{h}.
	\]
	Letting $h\to 0$ yields the differential equation
	\[
	g'(t)=g(t)\lambda (e^{-u}-1).
	\]
	The initial condition $g(0)=\mathrm{E}[e^{-uN_{0}}]=1$ determines a special solution of the equation 
	$$g(t)=L_{N_{t}}(u)=e^{\lambda t (e^{-u}-1)},$$
	which coincides with the Laplace transform of Poisson distribution $Pois(\lambda t)$. Since Laplace transform uniquely determines the distribution, we can thus conclude $N_{t}\sim Pois(\lambda t)$. Given any $r\ge 0$, define a stochastic process $N'_t=N_{r+t}-N_r$ and we can check that $(N'_t)_{t\ge 0}$ is also a counting process satisfying all the contitions in Definition 2.3. Hence by repeating the proof above we can show $N'_{t}\sim Pois(\lambda t)$, which is equivalent to Definition 2.1(iii).

	\item\noindent\underline{Definition 2.1 $\implies$ Definition 2.4}\\
	Let $T_n=\inf\{t\ge0:N_t= n\}$ for $n\in\mathbb{N}_+$. Note that given any $t\ge0$, $\{N_t=n\}=\{T_n\le t<T_{n+1}\}$. Thus we have
	\[
	N_t=\sum_{n=1}^{\infty}nI_{N_t=n}=\sum_{n=1}^{\infty}nI_{[T_n,T_{n+1})}(t).
	\]
	Let $X_1=T_1,X_{n}=T_{n}-T_{n-1}(n\ge2)$. Since $\mathrm{P}(X_1>t)=\mathrm{P}(N_t=0)=e^{-\lambda t}$, we see $X_1\sim Exp(\lambda)$. 
	Since
	\[
	\mathrm{P}(X_2>t|X_1=t_1)=\mathrm{P}(X_2>t|X_1=t_1)
	\]
	When $n\ge2$, since
	\begin{align*}
	&\mathrm{P}(X_n>t|X_{n-1}=t_{n-1},\cdots,X_1=t_1)\\
	=\;&\mathrm{P}(T_{n}-T_{n-1}>t|T_{n-1}-T_{n-2}=t_{n-1},\cdots,T_1=t_1)\qquad(\text{let }s_n=t_{n}+\cdots +t_1)\\
	=\;&\mathrm{P}(T_{n}>s_{n-1}+t|T_{n-1}=s_{n-1},\cdots,T_1=s_1)\\
	=\;&\mathrm{P}(N_{s_{n-1}+t}=n-1|N_{s_{n-1}}=n-1)\qquad(\text{memoryless property of }(N_t))\\
	=\;&\mathrm{P}(N_{s_{n-1}+t}-N_{s_{n-1}}=0|N_s=n-1)\\
	=\;&\mathrm{P}(N_{s_{n-1}+t}-N_{s_{n-1}}=0)\\
	=\;&e^{-\lambda t},
	\end{align*}
	it is plain to show that $\{X_i\}$ is sequence of independent random variable. Furthermore, we have $$\mathrm{P}(X_n>t)=\mathrm{E}[\mathrm{P}(X_n>t|X_{n-1},\cdots,X_1)]=e^{-\lambda t},$$
	which implies $X_i\text{ i.i.d }\sim Exp(\lambda)$,
	\item\noindent\underline{Definition 2.4 $\implies$ Definition 2.1}
	
	\noindent Clearly $N_0=0$ holds. Since $T_n=X_1+X_2+\cdots+X_n$ and $X_i\text{ i.i.d }\sim Exp(\lambda)$, we can deduce the jointly probability density function of $(T_1,T_2,\cdots,T_m)$ 
	\begin{align*}
	f_S(y_1,y_2,\cdots,y_m)&=f_X(y_1,y_2-y_1,\cdots,y_m-y_{m-1})\left|\frac{\partial(x_1,\cdots,x_m)}{\partial(y_1,\cdots,y_m)}\right|\\
	&=\lambda^m e^{-\lambda y_m}I_{\{0\le y_1<\cdots<y_m\}}.
	\end{align*}
	Thus for any $1\le j_1<j_2<\cdots<j_n$, the jointly probability density function of $(T_{j_1},T_{j_2},\cdots,T_{j_n})$ is
	\[
	\frac{y_1^{j_1-1}}{(j_1-1)!}\frac{(y_2-y_1)^{j_2-j_1-1}}{(j_2-j_1-1)!}\cdots\frac{(y_n-y_{n-1})^{j_n-j_{n-1}-1}}{(j_n-j_{n-1}-1)!}\lambda^{j_n}e^{-\lambda y_n}I_{\{0\le y_1<\cdots<y_n\}}
	\]
	Note $$N_t=\sum_{n=1}^{\infty}nI_{[T_n,T_{n+1})}(t)$$ implies $\{N_t=n\}=\{T_n\le t<T_{n+1}\}$. For any $n\in \mathbb{N}_+$ and any $0\le t_1<t_2<\cdots<t_n$, we have
	\begin{align*}
	&\ \mathrm{P}(N_{t_1}=j_1,N_{t_1}-N_{t_2}=j_2,\cdots,N_{t_{n}}-N_{t_{n-1}}=j_n)\\
	=&\ 
	\mathrm{P}(N_{t_1}=j_1,N_{t_1}=j_1+j_2,\cdots,N_{t_{n}}=j_1+\cdots+j_n)\quad(\text{let }k_n=j_1+\cdots j_n)\\
	=&\ \mathrm{P}(T_{k_1}\le t_1,T_{k_1+1}>t_1,T_{k_2}\le t_2,T_{k_2+1}>t_2,\cdots,T_{k_n}\le t_n,T_{k_n+1}>t_n)\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<y_{k_n}\le t_n<y_{k_n+1}} \frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_n}-y_{k_{n-1}+1})^{k_n-k_{n-1}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n+1}e^{-\lambda y_{k_n+1}}dy_1\cdots dy_{k_n+1}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<y_{k_n}\le t_n} \frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_n}-y_{k_{n-1}+1})^{k_n-k_{n-1}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}dy_{k_1}\cdots dy_{k_n}\int_{t_n}^{\infty}-de^{-\lambda y_{k_n+1}}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<t_{n-1}<y_{k_{n-1}+1}< t_n}\\
	&\frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_{n-1}}-y_{k_{n-2}+1})^{k_{n-1}-k_{n-2}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}e^{-\lambda t_n}dy_1\cdots dy_{k_{n-1}+1}\int_{y_{k_{n-1}+1}}^{t_n}d\frac{(y_{k_n}-y_{k_{n-1}+1})^{k_n-k_{n-1}-1}}{(k_n-k_{n-1}-1)!}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<t_{n-1}<y_{k_{n-1}+1}<t_n}\\
	&\frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_{n-1}}-y_{k_{n-2}+1})^{k_{n-1}-k_{n-2}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}e^{-\lambda t_n}dy_1\cdots dy_{k_{n-1}}\int_{t_{n-1}}^{t_n}-d\frac{(t_{n}-y_{k_{n-1}})^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}\\
	=&\ \int_{y_{k_1}\le t_1<y_{k_1+1}<\cdots<t_{n-1}<y_{k_{n-1}+1}<t_n}\\
	&\frac{y_{k_1}^{k_1-1}}{(k_1-1)!}\cdots\frac{(y_{k_{n-1}}-y_{k_{n-2}+1})^{k_{n-1}-k_{n-2}-2}}{(k_n-k_{n-1}-2)!}\lambda^{k_n}e^{-\lambda t_n}dy_1\cdots dy_{k_{n-1}}\frac{(t_{n}-t_{n-1})^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}\\
	=&\cdots\\
	=&\lambda^{k_n}e^{-\lambda t_n}\frac{t_{1}^{k_1}}{k_1!}\frac{(t_{2}-t_{1})^{k_2-k_{1}}}{(k_2-k_{1})!}\cdots\frac{(t_{n}-t_{n-1})^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}\\
	=&e^{-\lambda t_1}\frac{(\lambda t_1)^{j_1}}{j_1}e^{-\lambda(t_2- t_1)}\frac{(\lambda (t_2-t_1))^{j_2}}{j_2}\cdots e^{-\lambda(t_n-t_{n-1}) }\frac{(\lambda(t_{n}-t_{n-1}))^{j_n}}{j_n!}
\end{align*}
	Therefore, we conclude $N_{t_2}-N_{t_1},N_{t_3}-N_{t_2},\cdots,N_{t_n}-N_{t_{n-1}}$ are independent and for any $0\le s < t$, $N_t-N_s\sim Pois(\lambda(t-s))$.
\end{itemize}
\end{proof}

\begin{proposition}
	Let $(N_t)_{t\ge0}$ be a Poisson process.
	\begin{enumerate}
		\item $N_t\sim Pois(\lambda t)$, $\mathrm{E}[N_t]=\mathrm{Var}(N_t)=\lambda t$.
		\item For $0\le s\le t$, $\mathrm{E}[N_tN_s]=\lambda^2ts+\lambda s$, $\mathrm{Cov}(E_t,E_s)=\lambda s$.
		\item For $0\le s\le t$, $\mathrm{E}[N_t|N_s]=N_s+\lambda(t-s)$. So Poisson process is a submartingale.
		\item Poisson process is a Markov process. For $0\le t_1< t_2<\cdots<t_n$ and $0\le k_1\le k_2\le\cdots\le k_n$,
		\begin{align*} &\mathrm{P}(N_{t_n}=k_n|N_{t_{n-1}}=k_{n-1},\cdots,N_{t_1}=k_1)\\
		=\ &\mathrm{P}(N_{t_n}=k_n|N_{t_{n-1}}=k_{n-1})\\
		=\ &\mathrm{P}(N_{t_n}-N_{t_{n-1}}=k_n-k_{n-1})\\
		=\ &e^{-\lambda(t_n-t_{n-1}) }\frac{(\lambda(t_{n}-t_{n-1}))^{k_n-k_{n-1}}}{(k_n-k_{n-1})!}. 
		\end{align*}
	\end{enumerate}
\end{proposition}
\begin{proof}
	Apply \hyperlink{Definition 2.1(ii)}{Definition 2.1(ii)} and it is straightforward to show the properties.
\end{proof}

\section{Compound Poisson Process}
\begin{definition}[compound Poisson distribution]
	Suppose that
	$N\sim Pois(\lambda )$ and that $Z_{1},Z_{2},Z_{3},\cdots $ are i.i.d. random variables independent of $N$ with a probability measure $v(dy)$ on $\mathbb{R}$. Then the probability distribution of the sum of $N$ i.i.d. random variables
	\[
	Y=\sum _{n=1}^{N}Z_{n}
	\]
	is a \emph{compound Poisson distribution}.
\end{definition}


\begin{definition}[compound Poisson process]
	\emph{A compound Poisson process}, parameterised by a rate $\lambda >0$ and jump size distribution $v(dy)$, is a process $(Y_t)_{t\ge 0}$ given by
	\[
	Y_t=\sum _{n=1}^{N_t}Z_{n},
	\]
	where $(N_t)_{t\ge 0}$ is a Poisson process with rate $\lambda$, and $(Z_n)_{n\in \mathbb{N}_+}$ are independent and identically distributed random variables with distribution $v(dy)$, which are also independent of $(N_t)_{t\ge 0}$.
\end{definition}

\begin{proposition}
	Let $(Y_t)_{t\ge 0}$ be a compound Poisson process with a rate $\lambda$ and jump $(Z_n)_{n\in \mathbb{N}_+}$. For convenience, assume $Z_n\overset{d}{=}Z$ and $\mathrm{E}[Z^2]<+\infty$.
	\begin{enumerate}
		\item $\mathrm{E}[Y_t]=\lambda tE[Z]$.
		\item $\mathrm{Var}(Y_t)=\lambda tE[Z^2]$.
		\item The moment generating function $M_{Y_t}(a)=\mathrm{E}[e^{aY_t}]=e^{\lambda t(\mathrm{E}[e^{aZ}]-1)}=e^{\lambda t(M_Z(a)-1)}$
	\end{enumerate}
\end{proposition}
\begin{proof}\hspace*{1em}	
\begin{enumerate}
	\item Since $Z_n$ is independent of $N_t$, we have $$\mathrm{E}[Y_t]=\mathrm{E}[\mathrm{E}[Y_t|N_t]]=\mathrm{E}\left[\mathrm{E}\left[\left.\sum _{n=1}^{N_t}Z_{n}\right|N_t\right]\right]=\mathrm{E}\left[\sum _{n=1}^{N_t}\mathrm{E}\left[\left.Z_{n}\right|N_t\right]\right]=\mathrm{E}[N_tZ]=\mathrm{E}[N_t]\mathrm{E}[Z]=\lambda tE[Z].$$
	\item Since $Z_n$ is independent of $N_t$, by the law of total variance $\mathrm{Var}(Y_t)$ can be calculated as
	\begin{align*}
		\mathrm{Var}(Y_t)&=\mathrm{E}[\mathrm{Var}(Y_t|N_t)]+\mathrm{Var}(\mathrm{E}[Y_t|N_t])\\
		&=\mathrm{E}[N_t\mathrm{Var}(Z)]+\mathrm{Var}(N_t\mathrm{E}[Z])\\
		&=\mathrm{Var}(Z)\mathrm{E}[N_t]+\mathrm{E}[Z]^2\mathrm{Var}(N_t)\\
		&=\lambda t\mathrm{Var}(Z)+\lambda t\mathrm{E}[Z]^2\\
		&=\lambda t \mathrm{E}[Z^2].
	\end{align*}
	\item Make similar use of the dependence of $(Z_n)_{n\in\mathbb{N_+}}$ and $N_t$ to get
	\[
	\begin{aligned}
	\mathrm{E}\left[e^{aY_t}\right]&=\mathrm{E}\left[e^{a\left(Z_1+Z_2+\cdots+Z_{N_t}\right)}\right]\\
	&=\mathrm{E}\left[\mathrm{E}\left[\left.e^{a\left(Z_1+Z_2+\cdots+Z_{N_t}\right)}\right|N_t\right]\right]\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[\left.e^{a\left(Z_1+Z_2+\cdots+Z_{N_t}\right)}\right|N_t=n\right]\mathrm{P}\left(N_t=n\right)\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[\left.e^{a\left(Z_1+Z_2+\cdots+Z_{n}\right)}\right|N_t=n\right]e^{-\lambda t}\frac{\left(\lambda t\right)^n}{n!}\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[e^{aZ_1}e^{aZ_2}\cdots e^{aZ_{n}}\right]e^{-\lambda t}\frac{\left(\lambda t\right)^n}{n!}\\
	&=\sum_{n=0}^{\infty}\mathrm{E}\left[e^{aZ}\right]^ne^{-\lambda t}\frac{\left(\lambda t\right)^n}{n!}\\
	&=e^{-\lambda t}\sum_{n=0}^{\infty}\frac{\left(\lambda t\mathrm{E}\left[e^{aZ}\right]\right)^n}{n!}\\
	&=e^{\lambda t\left(\mathrm{E}\left[e^{aZ}\right]-1\right)}.
 	\end{aligned}
 	\]
\end{enumerate}
\end{proof}
Let $Y=(Y_t)_{t\ge 0}$ be a compound Poisson process with a rate $\lambda$ and jump $(Z_n)_{n\in \mathbb{N}_+}$. Let $T_n=\inf\{t\ge0:N_t= n\}$ be the time when the $n$th event happens. Then the Itô integral of a stochastic process $K$ with respect to $Y$ is
\[
\int_{0}^{t}KdY=\sum_{n=0}^{N_t}K_{T_n}(Y_{T_n}-Y_{T_n-})=\sum_{n=0}^{N_t}K_{T_n}Z_n.
\]


\section{Markov Chain}
\subsection{Discrete-time Markov Chain}
\begin{definition}[discrete-time Markov chain]
	A \emph{discrete-time Markov chain} on a countable state space $S$ is a sequence of random variables $X_0, X_1, X_2,\cdots$ with the Markov property, namely that $\forall n\ge0,\ \forall j,i_0,i_1,\cdots,i_n\in S$,	
	\[
	\mathrm{P}(X_{n+1}=j\mid X_{0}=i_{0},X_{1}=i_{1},\cdots,X_{n}=i_{n})=\mathrm{P}(X_{n+1}=j\mid X_{n}=i_{n}), 
	\] if both conditional probabilities are well defined, i.e., if $ 	\mathrm{P}(X_{0}=i_{0},\cdots ,X_{n}=i_{n})>0$.
\end{definition}	
Markov property may be interpreted as stating that the probability of moving to the next state depends only on the present state and not on the previous states. 

\noindent A discrete-time Markov chain $(X_n)_{n\ge0}$ is \emph{time-homogeneous} if
\[
\mathrm{P}(X_{n+2}=j\mid X_{n+1}=i)=\mathrm{P}(X_{n+1}=j\mid X_{n}=i)
\]
for all $n\ge0$ and all $i,j\in S$. We only focus on the discrete-time time-homogeneous Markov chain on a countable space if nothing is specified. 

\noindent One can consider the (possibly infinite-dimensional) \emph{transition matrix}	$P=(p_{ij})_{i,j\in S}$, where
\[
p_{ij}=\mathrm{P}(X_{n+1}=j\mid X_{n}=i)
\]
is called \emph{one-step transition probability}. By the law of total probability it is clear to see 
\[
\sum_{j\in S}p_{ij}=1,\quad\forall i\in S.
\]
Similarly we can define \emph{$n$-step transition matrix} $P^{(n)}=\left(p^{(n)}_{ij}\right)_{i,j\in S}$, where \emph{$n$-step transition probabilities} $p^{(n)}_{ij}$ is the probability that a process in state $i$ will be in state $j$ after $n$ additional transitions. That is,
\[
p^{(n)}_{ij}=\mathrm{P}(X_{k+n}=j\mid X_{k}=i).
\]
As is shown below, the Chapman–Kolmogorov equation enables us to calculate $n$-step transition matrix readily.
\begin{proposition}[Chapman–Kolmogorov equation]\hypertarget{Proposition 4.1}{}
Let $(X_n)_{n\ge0}$ be a discrete-time Markov chain on a countable state space $S$. The \emph{Chapman–Kolmogorov equation} states that
\[
p^{(n+m)}_{ij}=\sum_{k\in S}p^{(n)}_{ik}p^{(m)}_{kj},\quad\forall i,j\in S,
\]
or alternatively in matrix form
\[
P^{(n+m)}=P^{(n)}P^{(m)}.
\]
\end{proposition}
\begin{proof}
	\[
	\begin{aligned}
		p^{(n+m)}_{ij}&=\mathrm{P}(X_{n+m}=j\mid X_{0}=i)\\
		&=\sum_{k\in S}\mathrm{P}(X_{n+m}=j,X_{n}=k\mid X_{0}=i)\\
		&=\sum_{k\in S}\mathrm{P}(X_{n}=k\mid X_{0}=i)\mathrm{P}(X_{n+m}=j\mid X_{0}=i,X_{n}=k)\\
		&=\sum_{k\in S}\mathrm{P}(X_{n}=k\mid X_{0}=i)\mathrm{P}(X_{n+m}=j\mid X_{n}=k)\\
		&=\sum_{k\in S}p^{(n)}_{ik}p^{(m)}_{kj}
	\end{aligned}
	\]
\end{proof}
\noindent Of course $P^{(1)}=P$. Thus by iteration we show $P^{(n)}$ coincides with $P^n$. 

\noindent Let $\pi(n)=\left(p_i^{(n)}\right)_{i\in S}$ denote the probability distribution of $X_n$, where $p_i^{(n)}=\mathrm{P}(X_n=i)$. Then we have
\[
\pi(n)=\pi(0)P^n.
\]

\subsection{Continuous-time Markov Chain}

\begin{definition}[continuous-time Markov chain]
	A \emph{continuous-time Markov chain} on a countable state space $S$ is a stochastic $(X_t)_{t\ge 0}$ with the Markov property: for all $ n\ge0$, all $0\le t_0\le t_1\le \cdots\le t_n$, and all	$j,i_0,\cdots,i_n\in S$,
	\[
	\mathrm{P}(X_{t_{n+1}}=j\mid X_{t_0}=i_{1},X_{t_1}=i_{2},\cdots,X_{t_n}=i_{n})=\mathrm{P}(X_{t_{n+1}}=j\mid X_{t_n}=i_{n}), 
	\] if both conditional probabilities are well defined, i.e., if $ 	\mathrm{P}(X_{1}=i_{1},\cdots ,X_{n}=i_{n})>0$.
\end{definition}

\noindent A continuous-time Markov chain $(X_t)_{t\ge0}$ is \emph{time-homogeneous} if
\[
\mathrm{P}(X_{s+t}=j\mid X_{s}=i)=\mathrm{P}(X_{t}=j\mid X_{0}=i)=p_{ij}(t)
\]
for all $s,t\ge0$ and all $i,j\in S$. 
\begin{example}
	Poisson progress $(N_t)_{t\ge0}$ is continuous-time time-homogeneous Markov chain.
\end{example}
We only focus on the continuous-time time-homogeneous Markov chain on a countable space if nothing is specified.
In this case, transition probability $p_{ij}(t)$ depends on the interarrival $t$ from state $i$ to state $j$.
\noindent One can define the \emph{transition matrix}	$P(t)=(p_{ij}(t))_{i,j\in S}$ and we have the similar properties of $p_{ij}(t)$ like the concrete-time case.
\begin{proposition}
	Let $(X_t)_{t\ge0}$ be a continuous-time Markov chain on a countable state space $S$. 
	\begin{enumerate}
		\item For all $t\ge 0$ and all $j\in S$,
		\[
		\sum_{j\in S}p_{ij}(t)=1.
		\]
		\item The \emph{Chapman–Kolmogorov equation} states that
		\[
		p_{ij}(t+s)=\sum_{k\in S}p_{ik}(t)p_{kj}(s),\quad\forall i,j\in S,
		\]
		or alternatively in matrix form
		\[
		P(t+s)=P(t)P(s).
		\]
	\end{enumerate}
\end{proposition}
\begin{proof}\hspace{1em}
	\begin{enumerate}
	\item It follows by the law of total probability.
	\item Imitate the proof in \hyperlink{Proposition 4.1}{Proposition 4.1} and the result is straightforward.
	\end{enumerate} 
\end{proof}
\begin{definition}
A continuous-time Markov chain is \emph{regular} if it satisfy the following condition
\[
\lim_{t\to0}\;p_{ij}(t)=\delta_{ij}=
\begin{cases}
1,&i=j,\\
0,&i\ne j.
\end{cases}
\]
\end{definition}
Since $p_{ij}(0)=\mathrm{P}(X_{t}=j\mid X_{0}=i)=\delta_{ij}$, regularity implies $p_{ij}(t)$ is continuous at $t=0$. Furthermore, we have the following stronger results. We always assume continuous-time Markov chain is regular afterwards.
\begin{lemma}
	If a continuous-time Markov chain is regular, for any fixed $i,j\in S$, $p_{ij}(t)$ is uniformly continuous with respect to $t$. 
\end{lemma}
\begin{proof}
	Since when $h>0$
	\[
	\begin{aligned}
		p_{ij}(t+h)-p_{ij}(t)&=\sum_{k\in S}p_{ik}(h)p_{kj}(t)-p_{ij}(t)\\
		&= p_{ii}(h)p_{ij}(t)-p_{ij}(t)+\sum_{k\in S\setminus\{i\}}p_{ik}(h)p_{kj}(t)\\
		&= -(1-p_{ii}(h))p_{ij}(t)+\sum_{k\in S\setminus\{i\}}p_{ik}(h)p_{kj}(t),
	\end{aligned}
	\]
	we have
	\[
	\begin{aligned}
	&p_{ij}(t+h)-p_{ij}(t)\ge -(1-p_{ii}(h))p_{ij}(t)\ge-(1-p_{ii}(h)),\\
	&p_{ij}(t+h)-p_{ij}(t)\le\sum_{k\in S\setminus\{i\}} p_{ik}(h)p_{kj}(t)\le \sum_{k\in S\setminus\{i\}}p_{ik}(h)=1-p_{ii}(h),	
	\end{aligned}
	\]
	which implies
	\[
	|p_{ij}(t+h)-p_{ij}(t)|\le 1-p_{ii}(h).
	\]
	When $h<0$ in a similar way we can get
	\[
	|p_{ij}(t)-p_{ij}(t+h)|\le 1-p_{ii}(-h).
	\]
	In general
	\[
	|p_{ij}(t+h)-p_{ij}(t)|\le 1-p_{ii}(|h|).
	\]
	According to regular condition we conclude for any $t\ge0$,
	\[
	\lim_{h\to0}|p_{ij}(t+h)-p_{ij}(t)|=0,
	\]
	that is, $p_{ij}(t)$ is uniformly continuous with respect to $t$ on $[0,\infty)$.
\end{proof}
If $p_{ij}(t)$ is differentiable, define the \emph{transition rate} 
\[
q_{ij}=\left.\frac{dp_{ij}(t)}{dt}\right|_{t=0}=\lim\limits_{h\to 0}\frac{p_{ij}(h)-p_{ij}(0)}{h}.
\]
The $q_{ij}$ can be seen as measuring how quickly the transition from $i$ to $j$ happens. Then define the \emph{transition rate matrix} $Q=(q_{ij})_{i,j\in S}$ with dimensions equal to that of the state space. 

\noindent Since $P(0)=I$, it can be shown that
\[
\mathrm{P}(X_{t+h}=j\mid X_t=i)=p_{ij}(h)=\delta _{ij}+q_{ij}h+o(h). 
\]
As an intermediate consequence, if the state space $S$ is finite, we have   
\[
\sum_{j\in S}q_{ij}=0,
\]
which follows by
\[
1=\sum_{j\in S}p_{ij}(h)=\sum_{j\in S}[\delta _{ij}+q_{ij}h+o(h)]=1+\sum_{j\in S}q_{ij}h+o(h).
\]
\begin{theorem}
	Let $(X_t)_{t\ge0}$ be a continuous-time Markov chain on a countable state space $S$.
	\begin{enumerate}
		\item Kolmogorov forward equation:
		\[
		p_{ij}'(t)=\sum_{k\in S}p_{ik}(t)q_{kj}
		\]
		or alternatively
		\[
		P'(t)=P(t)Q(t).
		\]
		\item Kolmogorov backward equation:
		\[
		p_{ij}'(t)=\sum_{k\in S}q_{ik}p_{kj}(t)
		\]
		or alternatively
		\[
		P'(t)=Q(t)P(t).
		\]
	\end{enumerate}
\end{theorem}
\section{Martingale}
\subsection{Basic Notion}
\begin{definition}[conditional expectation]
	Let $X$ be a $\mathcal{F}$-measurable random variable on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ such that $\mathrm{E}[|X|] < \infty$. Given a $\sigma$-algebra $\mathcal{G}\subset\mathcal{F}$, a random variable $Z$ that is $\mathcal{G}$-measurable and satisfies 
	\[
	\mathrm{E}[X I_A] = \mathrm{E}[ZI_A] \quad\text{ for all }A \in \mathcal{G}
	\] 
	is called the \emph{conditional expectation} of $Y$ given $\mathcal{G}$ and is written as $\mathrm{E}(X\mid\mathcal{G})$.
\end{definition}
In probability theory, we show that conditional expectation exists and is unique up to absolutely surely equality. If not pointed out explicitly, all equalities and inequalities involving conditional expectation are considered to hold absolutely surely. 
\begin{proposition}
	Let $X$, $Y$, $X_n$ be integrable $\mathcal{F}$-measurable random variables on a probability space $(\Omega,\mathcal{F},\mathrm{P})$ and  $\mathcal{G}\subset\mathcal{F}$ be a $\sigma$-algebra.
	\begin{enumerate}  
		\item If $a$, $b$ are constants, then $\mathrm{E}[aX+bY\mid\mathcal{F}]=a\mathrm{E}[X\mid\mathcal{F}]+b\mathrm{E}[Y\mid\mathcal{F}]$.
		\item If $X$ equals a constant $a$, then $\mathrm{E}[X\mid\mathcal{F}]=a$.
		\item If $X\ge Y$,  then $\mathrm{E}[X\mid\mathcal{F}]\ge\mathrm{E}[Y\mid\mathcal{F}]$.
		\item $|\mathrm{E}[X\mid\mathcal{F}]|\le\mathrm{E}[|X|\mid\mathcal{F}]$.
		\item If $\phi$ is a convex function on $\mathbb{R}$ and $\phi(X)$ is integrable, then $\phi(\mathrm{E}[X\mid\mathcal{F}])\le\mathrm{E}[\phi(X)\mid\mathcal{F}]$.
		\item If $\lim\limits_{n\to\infty}X_n=X$ and $|X_n|\le X$, then $\lim\limits_{n\to\infty}\mathrm{E}[X_n\mid\mathcal{F}]=\mathrm{E}[X\mid\mathcal{F}]$.
		\item $\mathrm{E}[\mathrm{E}[X\mid\mathcal{F}]]=\mathrm{E}[X]$.
		\item $\mathrm{E}[\mathrm{E}[X\mid\mathcal{G}]\mid\mathcal{F}]=\mathrm{E}[\mathrm{E}[X\mid\mathcal{F}]\mid\mathcal{G}]=\mathrm{E}[X\mid\mathcal{G}]$
		\item If $X$ and $\mathcal{F}$ are independent, that is, whenever $A\in\sigma(X)$ and $B\in\mathcal{F}$, $\mathrm{P}(A\cap B)= \mathrm{P}(A) \mathrm{P}(B)$, then $\mathrm{E}[X\mid\mathcal{F}]=X$.
		\item If $Z$ is $\mathcal{F}$-measurable and $ZX$ is integrable, then $\mathrm{E}[ZX\mid\mathcal{F}]=Z\mathrm{E}[X\mid\mathcal{F}]$
	\end{enumerate}
\end{proposition}
\begin{definition}[filtration]
Let $(\Omega,\mathcal{F},\mathrm{P})$ be a probability space and let $T$ be a linearly ordered index set such as $\mathbb{N}$ or $\mathbb{R}_{\ge 0}$. For every $t\in T$ let $\mathcal{F}_{t}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. Then
\[
\mathbb{F} =(\mathcal{F}_{t})_{t\in T}
\]
is called a \emph{filtration} if $\mathcal {F}_{k}\subset \mathcal{F}_{\ell}$ for all $k\leq \ell$. 
\end{definition}
If $ \mathbb {F}=(\mathcal{F}_{t})_{t\in T }$ is a filtration, then $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ is called a \emph{filtered probability space}. A stochastic process $(X_{t})_{t\in T }$ is said to be defined on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ if $(X_{n})_{t\in T }$ is defined on $(\Omega,\mathcal{F},\mathrm{P})$ and adapted to the filtration $\mathbb{F} =(\mathcal{F}_{t})_{t\in T }$, that is, $\sigma(X_t)\subset \mathcal{F}_t$ for all $t\in T$. 

\noindent If $T=[0,\infty]$, the definition of $\mathcal{F}_{\infty}$ is usually specified as
\[
\mathcal{F}_{\infty}=\sigma\left(\bigcup_{t\ge0} \mathcal{F}_t\right)
\]

\begin{definition}[right-continuous filtration]
Given a filtration $\mathbb{F}=(\mathcal{F}_{t})_{t\in T}$, define
\[
\mathcal{F}_{t+}:=\bigcap_{s>t}\mathcal{F}_{s}\quad,\forall t\in T.
\]
Then $\mathbb{F}^+:=({\mathcal{F}}_{t+})_{t\in T}$ is a filtration. The filtration $\mathbb {F}$ is called \emph{right-continuous} if and only if $\mathbb{F}^{+}=\mathbb {F}$.
\end{definition}

\begin{definition}[complete filtration]
Let
\[\mathcal{N}_{P}:=\{A\subset 2^\Omega \mid A\subset B\text{ for a }B\text{ with }\mathrm{P}(B)=0\}
\]
be the set of all sets that are contained within a $\mathrm{P}$-null set. A filtration $\mathbb{F} =(\mathcal{F}_{t})_{t\in T}$ is called a \emph{complete filtration}, if every $\mathcal{F}_{t}$ contains $\mathcal{N}_{P}$. This is equivalent to $(\Omega,\mathcal{F}_{t},\mathrm{P})$ being a complete measure space for every $t\in T$.
\end{definition}

\begin{definition}[natural filtration]
	Let $X=(X_{t})_{t\ge 0}$ be a stochastic process on the probability space $(\Omega,\mathcal{F},\mathrm{P})$. Then 
	\[
	{\mathcal {F}}_{t}^X=\sigma (\{X_{s}:0\le s\le t\})
	\]
	is a $\sigma$-algebra and $(\mathcal {F}_{t}^X)_{t\ge0} $ is a filtration that $X$ is adapted to. We call $(\mathcal {F}_{t}^X)_{t\ge0} $ the \emph{natural filtration} induced by the stochastic process $X$. $(\mathcal {F}_{t}^X)_{t\ge0} $ is the minimum filtration which $X$ is adapted to.
\end{definition}

\begin{definition}[stopping time]
Let $\tau$  be a random variable defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T },\mathrm{P})$ with values in $T$. Then $\tau$  is called a \emph{stopping time} (with respect to the filtration $(\mathcal{F}_{t})_{t\in T}$), if the following condition holds:
\[
\forall t\in T,\ \{\tau \leq t\}\in {\mathcal {F}}_{t}
\]
or equivalently
\[
X_{t} :=1_{\tau\le t}=\left\{\begin{array}{ll}{1} & {\text { if }  \tau\le t } \\ {0} & {\text { if }\tau >t}\end{array}\right.
\]
is adapted to $(\mathcal{F}_{t})_{t\in T }$.
\end{definition}

\begin{definition}[stopping time in discrete-time case]
	Let $\tau$  be a random variable defined on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ with values in $\mathbb{N}\cup\{+\infty\}$. Then $\tau$  is called a stopping time (with respect to the filtration $(\mathcal{F}_{n})_{n\ge 0}$), if the following condition holds:
	\[
	\forall n\in\mathbb{N},\ \{\tau =n\}\in {\mathcal {F}}_{n}.
	\]
\end{definition}
\begin{example}
	Given a discrete-time stochastic process $(X_{n})_{n\ge 0}$ defined on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0 },\mathrm{P})$ and a Borel set $B$,
	\[
	\tau=\inf\{n\ge0:X_n\in B\}
	\]
	is a stopping time, called the \emph{first hitting time}. ($\inf\varnothing = \infty $)
\end{example}

\begin{definition}[stopping process]
	Let $X=(X_{t})_{t\ge 0}$ be a stochastic process on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\in T},\mathrm{P})$ and $\tau$ be a stopping time with respect to the filtration $(\mathcal{F}_{t})_{t\in T}$. The stopping process $X^\tau$ is defined as $(X_{\tau \wedge t})_{t\in T}$, where
	\begin{align*}
	X_{\tau \wedge t}:\Omega&\longrightarrow S\\
	\omega&\longmapsto X_{\tau(\omega) \wedge t}(\omega).
	\end{align*}
\end{definition}

\subsection{Discrete-time Martingale}
\begin{definition}[discrete-time martingale]
	A discrete-time stochastic process $M=(M_n)_{n\ge 0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ is a \emph{martingale} if it satisfies
	\begin{enumerate}
		\item For $n\ge0$, $\mathrm{E}[|M_n|]<+\infty$;
		\item For $n\ge0$, $\mathrm{E}[M_{n+1}\mid\mathcal{F}_n]=M_n$.
	\end{enumerate}
\end{definition}
\begin{definition}[discrete-time submartingale]
A discrete-time \emph{submartingale} is a stochastic process $M=(M_n)_{n\ge 0}$ consisting of integrable random variables satisfying for $n\ge0$
\[
\mathrm{E}[M_{n+1}\mid\mathcal{F}_n]\ge M_n.
\]
\end{definition}
\begin{definition}[discrete-time supermartingale]
	A discrete-time \emph{supermartingale} is a stochastic process $M=(M_n)_{n\ge 0}$ consisting of integrable random variables satisfying for $n\ge0$
	\[
	\mathrm{E}[M_{n+1}\mid\mathcal{F}_n]\le M_n.
	\]
\end{definition}

\begin{example}
	Suppose $(M_n)_{n\ge 0}$ is a martingale defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{n})_{n\ge0},\mathrm{P})$ and that $\phi:\mathbb{R}\to\mathbb{R}$ is convex. If $\phi(M_n)$ is integrable for $n\ge0$, then $\left(\phi(M_n)\right)_{n\ge 0}$ is a submartingale.
\end{example}

\subsection{Continuous-time Martingale}
\begin{definition}[continuous-time martingale]
	A continuous-time stochastic process $M=(M_t)_{t\ge 0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ is a \emph{martingale} if it satisfies
	\begin{enumerate}
		\item For $t\ge0$, $\mathrm{E}[|M_t|]<+\infty$;
		\item For $0\le t\le s<+\infty$, $\mathrm{E}[\,M_{s}\mid\mathcal{F}_t\,]=M_t$.
	\end{enumerate}
\end{definition}
\begin{definition}[continuous martingale]
	A continuous-time martingale $M=(M_t)_{t\ge 0}$ defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ is continuous if the paths of $M$ are almost surely continuous. That is, there exists an $\Omega_{0} \subset \Omega$ with $P\left(\Omega_{0}\right)=1$ such that for all $\omega \in \Omega_{0}$ the function
	\begin{align*}
	\gamma_\omega:[0, \infty)&\longrightarrow\mathbb{R}\\
	t&\longmapsto X_{t}(\omega)
	\end{align*}
	is continuous.
\end{definition}

\begin{definition}[uniform integrability]
	A class $\mathcal {C}$ of random variables is called \emph{uniformly integrable} if given $\varepsilon >0$, there exists $K\in [0,\infty )$ such that 
	\[
	\mathrm{E}\left(|X| 1_{|X| \geq K}\right) \leq \varepsilon \text { for all } X \in \mathcal{C}.
	\]	
\end{definition}

\begin{theorem}[Doob's maximal inequalities in continuous time]
If $M=(M_{t})_{t\ge0}$ is a continuous nonnegative submartingale on $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ and $\lambda>0,$ then for all $p \geq 1$ we have
\[
\lambda^{p} \mathrm{P}\left(\sup_{0 \le t \le T} M_{t}>\lambda\right) \le \mathrm{E}\left[M_{T}^{p}\right]
\]
and, if $M_{T} \in L^{p}(\Omega,\mathcal{F},\mathrm{P})$ for some $p>1,$ then we also have
\[
\left\|\sup_{0 \leq t \leq T} M_{t}\right\|_{p} \leq \frac{p}{p-1}\left\|M_{T}\right\|_{p}.
\]
\end{theorem}

\begin{definition}[$L^p$ bounded martingale]
	A martingale $M=(M_t)_{t\ge0}$ is said to be $L^p$ \emph{bounded} if 
	$$
	\sup _{t \geq 0} \mathrm{E}\left[|M_{t}|^{p}\right]<\infty.
	$$
\end{definition}


\begin{theorem}[martingale convergence theorems in continuous time] Let $M=(M_{t})_{t\ge0}$ be a continuous martingale on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$. If $M$ satisfies $\mathrm{E}\left[\left|M_{t}\right|^{p}\right] \leq B<\infty$ for some $p>1$ and all $t \geq 0,$ then there exists a random variable $M_{\infty}\in L^p(\Omega,\mathcal{F},\mathrm{P})$ with $\mathrm{E}\left[\left|M_{\infty}\right|^p\right] \leq B$ such that
\[
\mathrm{P}\left(\lim _{t \rightarrow \infty} M_{t}=M_{\infty}\right)=1 \text { and } \lim _{t \rightarrow \infty}\left\|M_{t}-M_{\infty}\right\|_{p}=0.
\]
Also, if $M$ satisfies $\mathrm{E}\left[\left|M_{t}\right|\right]\le B<\infty$ for all $t \ge 0,$ then there exists a random variable $M_{\infty}\in L^1(\Omega,\mathcal{F},\mathrm{P})$ with $\mathrm{E}\left[\left|M_{\infty}\right|\right] \leq B$ such that
\[
\mathrm{P}\left(\lim _{t \rightarrow \infty} M_{t}=M_{\infty}\right)=1.
\]
\end{theorem}
\begin{proposition}
All the $L^2$ bounded martingales on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ constitute a Hilbert space $H^2$, where the inner product is defined as $\langle M, N\rangle_{H^{2}}:=\left\langle M_{\infty}, N_{\infty}\right\rangle_{L^{2}}=E\left[M_{\infty} N_{\infty}\right]$. All the $L^2$ bounded continuous martingales on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ constitute a Hilbert space $H^2_c$, which is a closed subspace of $H^2$.
\end{proposition}

\begin{definition}[quadratic variation]	
		Suppose that $(X_t)_{t\ge 0}$ is a real-valued stochastic process defined on filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$. A stochastic process $Q=(\langle X\rangle_{t})_{t\ge0}$ is said to be the \emph{quadratic variation} of $(X_t)_{t\ge 0}$ if for all $t\ge0$, for all $\varepsilon>0$,
	\[
	\lim_{\left\Vert P_{[0,t]}\right\Vert \rightarrow 0}\mathrm{P}\left(\,\left|\sum _{k=1}^{n}(X_{t_{k}}-X_{t_{k-1}})^{2}-\langle X\rangle_{t}\right|>\varepsilon\right)=0
	\] 
	where $P_{[0,t]}=\{(t_0,\cdots,t_n):0=t_0<t_1<\cdots<t_n=t\}$ ranges over partitions of the interval $[0,t]$ and the norm of the partition $P_{[0,t]}$ is the length of the longest of these subintervals, namely
	\[
	\left\Vert P_{[0,t]}\right\Vert=\max\limits_{1\le k\le n}{|t_k-t_{k-1}|}.
	\]
\end{definition}

\begin{definition}[bracket process]	
	The bracket process of two processes $X$ and $Y$ is
	\[
	\langle X, Y\rangle_{t}:=\frac{1}{4}\left(\langle X+Y\rangle_{t}-\langle X-Y\rangle_{t}\right).
	\] 
\end{definition}

\begin{proposition}
	All the $L^2$ bounded martingales on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ constitute a Hilbert space $H^2$. All the $L^2$ bounded continuous martingales on the filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\ge0},\mathrm{P})$ constitute a Hilbert space $H^2_c$, which is closed subspace of $H^2$.
\end{proposition}








\section{Brownian Motion}
\begin{definition}[Brownian motion]
	A stochastic process $(B_t)_{t\ge0}$ is called a \emph{Brownian motion} if
	\begin{enumerate}
		\item $B_0=0$.
		\item $(B_t)_{t\ge0}$ has continuous path, that is $t\mapsto B_t$ is almost surely continuous.
		\item $(B_t)_{t\ge0}$ has independent and stationary increments.
		\item For $t>0$, $B_t\sim N(0,t)$.
	\end{enumerate}
\end{definition}

\begin{definition}[Gaussian process]
A stochastic process $(X_t)_{t\in T}$ is a \emph{Gaussian process} if and only if for every finite set of indices $t_{1},\ldots ,t_{n}$ in the index set $T$, $(X_{t_1},X_{t_2},\cdots,X_{t_n})$ follows multivariate normal distribution $N(\mu,\Sigma)$.
\end{definition}

\begin{theorem}
	$B=(B_t)_{t\ge0}$ is a Brownian motion if and only if $B$ is a Gaussian process satisfying
	\begin{enumerate}
		\item $B_0=0$,
		\item $B$ has continuous paths,
		\item For all $t\ge0$, $\mathrm{E}[B_t]=0$,
		\item For all $s,t\ge0$, $\mathrm{E}[B_sB_t]=s\wedge t$.
	\end{enumerate}
\end{theorem}
\begin{proposition}
	Let $B=(B_t)_{t\ge0}$ be a Brownian motion.
	\begin{enumerate}
	\item For $k\ge1$, $\mathrm{E}[B_t^{2k-1}]=0$, $\mathrm{E}[B_t^{2k}]=t^{k}(2k-1)!!$.
	\item $B$ is a Markov process. 
	\item $B$ is a martingale.
	\end{enumerate}
\end{proposition}

\begin{theorem}
	The quadratic variation of a Brownian motion $B$ exists, and is given by $[B]_t = t$. 
\end{theorem}
\begin{proof} 
	Given a partition $P$ of the interval $[0,t]$, we have
	\[
	\begin{aligned}
	\mathrm{E}\left[\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}\right]&=\sum _{k=1}^{n}\mathrm{E}\left[(B_{t_{k}}-B_{t_{k-1}})^{2}\right]\\
	&=\sum _{k=1}^{n}(t_{k}-t_{k-1})\\
	&=t
    \end{aligned}
    \]
    and
    \[
    \begin{aligned}
	\mathrm{Var}\left(\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}\right)&=\sum _{k=1}^{n}\mathrm{Var}\left((B_{t_{k}}-B_{t_{k-1}})^{2}\right)\\
	&=\sum _{k=1}^{n}\mathrm{E}\left[(B_{t_{k}}-B_{t_{k-1}})^{4}\right]-\sum _{k=1}^{n}\left(\mathrm{E}\left[\left(B_{t_{k}}-B_{t_{k-1}}\right)^{2}\right]\right)^{2}\\
	&=\sum_{k=1}^{n}3(t_{k}-t_{k-1})^2-\sum _{k=1}^{n}\left(t_{k}-t_{k-1}\right)^{2}\\
	&=2\sum_{k=1}^{n}(t_{k}-t_{k-1})^2\\
	&\le 2\Vert P\Vert\sum_{k=1}^{n}(t_{k}-t_{k-1})\\
	&= 2\Vert P\Vert t.
	\end{aligned}
	\]
	Since
	\[
	\lim\limits_{\Vert P\Vert\to0 }\mathrm{E}\left[\left(\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}-t\right)^2\right]=\lim\limits_{\Vert P\Vert \to0}\mathrm{Var}\left(\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}\right)\le \lim\limits_{\Vert P\Vert\to0 }2\Vert P\Vert t=0,
	\]
	we conclude 
	\[
	[B]_t=\lim_{\Vert P\Vert \rightarrow 0}\sum _{k=1}^{n}(B_{t_{k}}-B_{t_{k-1}})^{2}=t\quad\text{a.e.}
	\]
\end{proof}
\newpage

\section*{Appendix}
\subsection*{1.Properties of Common Distributions}
\makegapedcells
\setcellgapes{3pt}
\newcommand{\minitab}[2][l]{\begin{tabular}{#1}#2\end{tabular}} 
%\newsavebox{\mybox}
%\newcolumntype{X}[1]{>{\begin{lrbox}{\mybox}}c<{\end{lrbox}\makecell[#1]{\fbox{\usebox\mybox}}}}
%\renewcommand\arraystretch{1.5}
\begin{table}[H]
	\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\rowcolor[HTML]{C0C0C0} 
			\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Distribution}& \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}pmf $\mathrm{P}(X=k)$}& \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Support}   & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Variance} \\ \hline
			Bernoulli $B(1,p)$&$p^{k}(1-p)^{1-k}$&$\{0,1\}$&$p$&$p(1-p)$\\  \hline			
			Binomial $B(n,p)$ &${n \choose k}\,p^{k}(1-p)^{n-k}$&$\{0,\cdots, n\}$&$np$&$np(1-p)$ \\   \hline
			Negative Binomial $NB(r,p)$&${k+r-1 \choose k} (1-p)^{r}p^{k}$&$\mathbb{N}$&${\dfrac{pr}{1-p}}$&${\dfrac {pr}{(1-p)^{2}}}$\\ \hline
			Poisson $Pois(\lambda )$ &$\dfrac{\lambda ^{k}e^{-\lambda}}{k!}$
			&$\mathbb{N}$&$\lambda$&$\lambda$\\ \hline
			Geometric $Geo(p)$&$(1-p)^{k-1}p$&$\mathbb{N}_+$&$\dfrac{1}{p}$&$\dfrac{1-p}{p^2}$\\  \hline
			Hypergeometric $H(N,K,n)$&$\dfrac{{K \choose k}{{N-K}\choose {n-k}}}{{N \choose n}}$&$\{0,\cdots
			,\min{(n,K)}\}$&$n\dfrac{K}{N}$&$n\dfrac{K}{N}\dfrac{N-K}{N}\dfrac{N-n}{N-1}$\\ \hline
			Uniform (discrete) $DU(a,b)$&$\dfrac{1}{n}$&$\{a,a+1,\dots,b\}$&$\dfrac{a+b}{2}$&$\dfrac{(b-a+1)^{2}-1}{12}$\\ \hline
		\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Distribution}& \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}pdf}  & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Mean} & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Variance} \\ \hline
		Degenerate $\delta_a$&$I_{\{a\}}(x)$&$a$&0 \\ \hline
		Uniform (continuous) $U(a,b)$&$\dfrac{1}{b-a}I_{[a,b]}(x)$&$\dfrac{a+b}{2}$&$\dfrac{(b-a)^{2}}{12}$\\ \hline
		Exponential $Exp(\lambda)=\Gamma(1,\lambda )$&$\lambda e^{-\lambda x}I_{[0,+\infty)}(x)$&$\lambda^{-1}$ &$\lambda^{-2}$\\ \hline
		Normal $N(\mu ,\sigma ^{2})$&${\dfrac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}$&$\mu$&$\sigma^2$\\ \hline
		Gamma $\Gamma (\alpha,\beta )$&$\dfrac{\beta ^{\alpha }}{\Gamma (\alpha )}x^{\alpha -1}e^{-\beta x}I_{(0,+\infty)}(x)$&$\dfrac{\alpha}{\beta}$&$\dfrac{\alpha}{\beta^2}$ \\ \hline
		Chi-squared $\chi^{2}_k=\Gamma (\frac{k}{2},\frac{1}{2})$ &$\dfrac{1}{2^{k/2}\Gamma (k/2)}\;x^{k/2-1}e^{-x/2}I_{(0,+\infty)}(x)$&$k$&$2k$ \\ \hline
		Student's t $t_{\nu}$&$\dfrac { \Gamma \left( \frac { \nu + 1 } { 2 } \right) } { \sqrt { \nu \pi } \Gamma \left( \frac { \nu } { 2 } \right) } \left( 1 + \dfrac { x ^ { 2 } } { \nu } \right) ^ { - \frac { \nu + 1 } { 2 } }$&0&$\dfrac { \nu } { \nu - 2 }$ for $\nu > 2$
		\\ \hline
	\end{tabular}
\end{table}

\subsection*{2.Generating Function \& Characteristic Function}

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		\rowcolor[HTML]{C0C0C0} 
		\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}Distribution} &  \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Moment-generating function } & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}Characteristic function} \\ \hline
		Degenerate $\delta_a$ &${ e ^ { t a } }$ &$ e ^ { i t a }$  \\ \hline
		Bernoulli $B(1,p)$  &${ 1 - p + p e ^ { t } }$ & ${ 1 - p + p e ^ { i t } }$ \\  \hline
		\multirow{2}{*}[-3pt]{Geometric $Geo(p)$} &${ \dfrac { p e ^ { t } } { 1 - ( 1 - p ) e ^ { t } } },$& \multirow{2}{*}[-3pt]{$\dfrac{ pe^{it} }{1 - ( 1 - p ) e^ { it } } $} \\ 
		 &$ t < - \operatorname { l n } ( 1 - p )$& \\  \hline
		Binomial $B(n,p)$ &${ ( 1 - p + p e ^ { t } ) ^ { n } }$ & $ ( 1 - p + p e ^ { i t } ) ^ { n } $ \\   \hline
		Negative Binomial $NB(r,p)$  &$ \dfrac { ( 1 - p ) ^ { r } } { ( 1 - p e ^ { t } ) ^ { r } } $ & $ \dfrac { ( 1 - p ) ^ { r } } { ( 1 - p e ^ { i t } ) ^ { r } } $\\ \hline
		Poisson $Pois(\lambda )$ &$ e ^ { \lambda \left( e ^ { t } - 1 \right) } $ & $ e ^ { \lambda \left( e ^ { i t } - 1 \right) } $ \\ \hline
		Uniform (continuous) $U(a,b)$&$\begin{cases}{\dfrac{\mathrm {e} ^{tb}-\mathrm{e}^{ta}}{t(b-a)}}&{\text{for }}t\neq 0\\1&{\text{for }}t=0\end{cases}$ & $\begin{cases}{\dfrac{\mathrm {e} ^{itb}-\mathrm{e}^{ita}}{it(b-a)}}&{\text{for }}t\neq 0\\1&{\text{for }}t=0\end{cases}$\\ \hline
		Uniform (discrete) $DU(a,b)$&$ \dfrac { e ^ { a t } - e ^ { ( b + 1 ) t } } { ( b - a + 1 ) \left( 1 - e ^ { t } \right) } $ & $ \dfrac { e ^ { i t \mu } } { ( b - a + 1 ) \left( 1 - e ^ { i t } \right) } $\\ \hline
		Laplace $L(\mu ,b)$ &$\dfrac { e ^ { t \mu } } { 1 - b ^ { 2 } t ^ { 2 } } ,\ | t | < 1 / b $ & $ \dfrac { e ^ { i t \mu } } { 1 + b ^ { 2 } t ^ { 2 } } $ \\ \hline
		Normal $N(\mu ,\sigma ^{2})$&$ e ^ { t \mu + \frac { 1 } { 2 } \sigma ^ { 2 } t ^ { 2 } } $ & $ e ^ { i t \mu - \frac { 1 } { 2 } \sigma ^ { 2 } t ^ { 2 } } $\\ \hline
		Chi-squared $\chi^{2}_k$&$( 1 - 2 t ) ^ { - \frac { k } { 2 } } $&$ ( 1 - 2 i t ) ^ { - \frac { k } { 2 } } $ \\ \hline
		Noncentral chi-squared $\chi^{2}_k(\lambda)$&$e^{\lambda t/( 1 - 2 t ) }( 1 - 2 t )^{- \frac{ k } { 2 } } $ & $ e ^ { i \lambda t / ( 1 - 2 i t ) } ( 1 - 2 i t ) ^{ - \frac { k } { 2 } } $ \\ \hline
		Gamma $\Gamma (\alpha,\beta )$&$\left(1-{\dfrac{t}{\beta }}\right)^{-\alpha },\ t<\beta $ & $\left(1-{\dfrac{it}{\beta }}\right)^{-\alpha }$\\ \hline
		Exponential $Exp(\lambda )$&$\dfrac{\lambda}{\lambda-t}, t < \lambda$ & $\dfrac{\lambda}{\lambda-it}$\\ \hline
		Multivariate normal $N(\bm{\mu } ,\mathbf{\Sigma })$&$e ^ { \mathbf{t} ^ { T } \left( \bm{\mu} + \frac { 1 } { 2 } \mathbf{\Sigma } \mathbf{t} \right) }$&$e ^ { \mathbf{t} ^ { T } \left( i\bm{\mu} -\frac { 1 } { 2 } \mathbf{\Sigma } \mathbf{t} \right) }$ \\ \hline
		Cauchy $Cauchy(\mu ,\theta )$&Does not exist&$e ^ { i t \mu - \theta | t | }$\\ \hline
		Multivariate Cauchy &\multirow{2}{*}[-1.5pt]{Does not exist} &\multirow{2}{*}[-1.5pt]{$e ^ { i \mathbf { t } ^ { \mathrm { T } } \boldsymbol { \mu } - \sqrt { \mathbf { t } ^ { \mathrm { T } } \boldsymbol { \Sigma } \mathbf { t } } }$}\\
		$MultiCauchy(\bm{\mu } ,\mathbf{\Sigma })$&&\\ \hline
	\end{tabular}
\end{table}


\end{document}
